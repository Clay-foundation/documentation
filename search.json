[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clay documentation",
    "section": "",
    "text": "Clay is a foundational model of Earth using Earth Observation data. As the AI Deep Learning architecture, it uses an expanded visual transformer upgraded to understant geospatial and temporal relations on Earth data, from any instrument/spectral data. The AI self-supervised fundational task is a Masked Autoencoder (MAE) approach for training.\nThe Clay model primarily functions in two ways: first, by directly generating semantic embeddings for tasks like similarity searches, and second, through fine-tuning its outputs with additional data labels. This fine-tuning supports various tasks, including classification (e.g. flood detection and deforestation monitoring), regression (e.g. estimating carbon stock or crop yields), and generative tasks such as creating RGB imagery from SAR data. Moreover, users can further enhance model performance by incorporating higher-resolution data.\nThis documentation uses nbdev, which combines documentation, code samples and an SDK. This means that every page is also a python notebook anyone can use, with practical code examples for each functionality, and use case. Moreover, you can install pip install clay and use the same functions.\nClay is open source, open data and open for business."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Clay documentation",
    "section": "",
    "text": "Clay is a foundational model of Earth using Earth Observation data. As the AI Deep Learning architecture, it uses an expanded visual transformer upgraded to understant geospatial and temporal relations on Earth data, from any instrument/spectral data. The AI self-supervised fundational task is a Masked Autoencoder (MAE) approach for training.\nThe Clay model primarily functions in two ways: first, by directly generating semantic embeddings for tasks like similarity searches, and second, through fine-tuning its outputs with additional data labels. This fine-tuning supports various tasks, including classification (e.g. flood detection and deforestation monitoring), regression (e.g. estimating carbon stock or crop yields), and generative tasks such as creating RGB imagery from SAR data. Moreover, users can further enhance model performance by incorporating higher-resolution data.\nThis documentation uses nbdev, which combines documentation, code samples and an SDK. This means that every page is also a python notebook anyone can use, with practical code examples for each functionality, and use case. Moreover, you can install pip install clay and use the same functions.\nClay is open source, open data and open for business."
  },
  {
    "objectID": "index.html#where-is-what",
    "href": "index.html#where-is-what",
    "title": "Clay documentation",
    "section": "Where is what",
    "text": "Where is what\n\nOur website is madewithclay.org.\nThe Clay model code lives on Github. License: Apache.\nThe Clay model weights live on Huggin Face. License: OpenRAIL-M.\nThe Clay documentation lives on this site. License: CC-BY.\nThe Clay SDK lives on PyPi. License: Apache.\nWe maintain a set of embeddings on Source Cooperative. License: ODC-BY."
  },
  {
    "objectID": "index.html#how-to-use-clay",
    "href": "index.html#how-to-use-clay",
    "title": "Clay documentation",
    "section": "How to use Clay",
    "text": "How to use Clay\nThe model can be used in two main ways:\n\nDirectly, use it to make inference. See Model\n\nCheck and run Benchmarks on the model. See Benchmarks\n\nGenerating semantic embeddings. E.g. for Similarity search. See Embeddings.\nFine-tunning the model for other tasks, or for other input data. E.g. flood detection, crop yields, … See Fine-tunning."
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Clay documentation",
    "section": "How to contribute",
    "text": "How to contribute\nClay is an open source project, and we welcome contributions of all kinds.\nThe Documentation, python package and notebooks are all the same NBdev project, located here.\n\nNote: If you want to contribute to the model code, please check the model repository.\n\nTo install the nbdev project locally, you can use:\ngit clone git@github.com:Clay-foundation/documentation.git\ncd documentation\npip install nbdev\nnbdev_install_git_hooks\nAfter you make changes, you can export the notebooks into both the package, rendered documentation and clean jupyter notebook execution metadata with:\nnbdev_prepare\nIf you want to preview the documentation locally, you can use:\nnbdev_preview\nTo run the test locally, you need to install Github CLI and act extension sudo gh extension install nektos/gh-act.\nThe “Clay model releases” folder uses a lot of resources to document the version releases. To run these you also need access to the S3 bucket with outputs and all the embeddgins. You will need a local file (e.g. .secrets) with the AWS credentials to read the Clay buckets. Remember to confirm this file is on .gitignore to avoid commiting it.\nThen you can run the tests with:\ngh act --secret-file .secrets\n–\nClay is a fiscally sponsored project of Radiant Earth, a USA registered 501(c)3 non-profit."
  },
  {
    "objectID": "Clay Model releases/clay roadmap.html",
    "href": "Clay Model releases/clay roadmap.html",
    "title": "Clay Model roadmap",
    "section": "",
    "text": "–\nClay Model v0 was released in Dec’23.\n–\nClay Model v1 is expected to be released in Q1 2024. High-level goals include:\n\nMore trainning time on more spatial and temporal coverage of Earth EO archives.\nMore robust handling of data, go beyond [Sentinel-2, Sentinel-1, DEM]\nMore emphasis on semantic anchors or Earth.\nBetter QA, testing and benchmarking.\nBetter finetune documentation."
  },
  {
    "objectID": "finetunning.html",
    "href": "finetunning.html",
    "title": "Finetuning",
    "section": "",
    "text": "Fine-tuning refers to a process in machine learning where a pre-trained model is further trained on a specific dataset or task. Clay is designed as a pre-trained model, utilizing a vast collection of historical data across various locations and times on Earth. It employs general self-supervised tasks, focusing on the undifferentiated heavy lifting useful across various fine-tuning tasks. E.g. shapes, edges, colors, patterns, … Model starts from random values, so these basic features are very likely to be useful for many tasks.\nFine-tunning is thus: * Faster: The model starts with many learned features and only needs to learn the nuances of the new data. * More accurate: The model, having been trained on a large dataset, has learned general features that might be difficult to acquire from a smaller, specific dataset. * Resource efficient: Users don’t need to start from scratch and train a model from the ground up. Instead, they can use a pre-trained model and resume training on their specific needs. Thus requiring less computational power, less energy consumption, reduced carbon emissions, and cost savings. * Strategic: Clay pre-trained models create a common baseline, available to anyone. This public asset lets you focus directly on creating value for the specific location, times and tasks that matter to you."
  },
  {
    "objectID": "finetunning.html#types-of-fine-tuning-with-clay",
    "href": "finetunning.html#types-of-fine-tuning-with-clay",
    "title": "Finetuning",
    "section": "Types of Fine-Tuning with Clay",
    "text": "Types of Fine-Tuning with Clay\n\nTransfer Learning: This involves taking a pre-trained Clay model and continuing its training with a specific dataset. It’s the simplest form of fine-tuning, especially useful for tasks like extracting model embeddings or maintaining the same autoencoder task.\nFull Fine-Tuning: Here, a pre-trained Clay model is further trained with the same or different data for a different task (like a classification task). Often, the weights of the early layers are retained or “frozen,” and new layers are added to the model’s end. Freezing early layers prevents “catastrophic forgetting”, where the model overwrites basic semantics instead of building upon them.\nLinear Probing: In this method, the pre-trained model’s weights are completely frozen, and only a linear classifier is trained on top of these features. This is a very fast technique, given that it only train one final layer. It is also somewhat limited task that don’t need new complex semantics. This technique is actually used while training the Clay model, as a way to check the quality of the features learned by the model, as it learns.\nLoRA (Low-Rank Adaptation): LoRA is similar to linear probing, in which all the model is kept frozen. Instead of training an added linear classifier at the output end of the model, LoRA inserts small trainable matrices inside the model. While finetuned there are only a few parameters to train so it is much faster that full fine-tune, but it also effectively changes the internal semantics. It is thus in between linear probing and full fine-tuning, in terms of speed and accuracy.\n\nLet’s take a look at how we are finetuning on the benchmark datacube-adapted Cloud to Street - Microsoft Flood Dataset. As a reminder, that is a downstream segmentation task for identifiying water pixels in recorded flood events. It’s a binary segmentation problem, specifically.\nWe process the datacubes into batches formatted in the way the pretrained Clay model expects, with the addition of information for label images as well. Here’s an example subset of a batch dictionary:\n{'labels': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n 'pixels': tensor([[[[-0.5994, -0.6108, -0.6034,  ..., -0.5610, -0.5590, -0.5614],\n           [-0.5767, -0.5950, -0.6004,  ..., -0.5619, -0.5536, -0.5610],\n           [-0.5841, -0.5762, -0.5930,  ..., -0.5491, -0.5304, -0.5373],\n           ...,\n           [-0.5087, -0.5447, -0.4351,  ..., -0.6162, -0.6083, -0.6044],\n           [-0.4184, -0.5432, -0.5003,  ..., -0.6108, -0.6128, -0.6073],\n           [-0.2496, -0.5348, -0.5225,  ..., -0.6137, -0.6167, -0.6128]],\n\n          [[-0.6371, -0.6435, -0.6425,  ..., -0.5834, -0.5898, -0.5923],\n           [-0.6296, -0.6410, -0.6385,  ..., -0.5794, -0.5983, -0.5958],\n           [-0.6167, -0.6177, -0.6182,  ..., -0.5545, -0.5913, -0.5834],\n           ...,\n           [-0.4800, -0.5153, -0.4308,  ..., -0.6525, -0.6410, -0.6331],\n           [-0.4104, -0.5034, -0.4318,  ..., -0.6331, -0.6226, -0.6087],\n           [-0.2404, -0.5222, -0.4522,  ..., -0.6231, -0.6241, -0.6177]],\n\n          [[-0.7068, -0.7217, -0.7101,  ..., -0.6118, -0.6178, -0.6290],\n           [-0.7087, -0.7022, -0.6924,  ..., -0.6141, -0.6146, -0.6234],\n           [-0.7017, -0.6998, -0.6831,  ..., -0.5927, -0.6085, -0.6104],\n           ...,\n           [-0.5563, -0.5480, -0.4571,  ..., -0.7106, -0.7045, -0.6933],\n           [-0.4725, -0.5526, -0.4781,  ..., -0.6975, -0.6789, -0.6807],\n           [-0.3117, -0.4995, -0.5000,  ..., -0.6952, -0.6835, -0.6845]],\n\n          ...,\n          ]),\n 'bbox': tensor([[ 661415., 5369305.,  666535., 5374425.]], dtype=torch.float64),\n 'epsg': tensor([32633], dtype=torch.int32),\n 'date': ['2020-10-20'],\n 'latlon': tensor([[-0.8192, -0.7854]]),\n 'timestep': tensor([[-1.2217,  2.7132, -2.4086]]),\n 'source_url': ['S2A_L2A_20201022T100051_N0209_R122_T33UXP_20201022T111023_06144-02560_S1B_IW_GRDH_1SDV_20201020T164222_20201020T164247_023899_02D6C4_rtc']}\nBatches of dictionaries like this run through the Clay model’s encoder to generate embeddings, such as this:\n\n\n\nembedding_ex\n\n\nfrom batches with image bands such as:\n\n\n\nband_red_ex\n\n\nand labels:\n\n\n\nlabels_ex\n\n\nThese embeddings are reshaped from shape batch size * (band groups length * number of patches) * embedding size to batch size * (band groups length * embedding size) * patch height * patch width before being passed to a series of 2D convolutional transpose and ReLU layers in a downstream decoder network.\nThat decoder network is the core of the downstream task. In a forward pass, it ingests the embeddings, runs them through those layers and computes a loss value with respect to the labels. The loss is back-propagated and the decoder gradually finetunes itself to the downstream dataset. Here’s a peek at the decoder layers:\nModel(\n  (decoder): Sequential(\n    (0): Conv2d(4608, 64, kernel_size=(1, 1), stride=(1, 1))\n    (1): Upsample(scale_factor=2.0, mode='nearest')\n    (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Upsample(scale_factor=2.0, mode='nearest')\n    (5): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Upsample(scale_factor=2.0, mode='nearest')\n    (8): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Upsample(scale_factor=2.0, mode='nearest')\n    (11): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): Upsample(scale_factor=2.0, mode='nearest')\n  )\n)\nNote the absence of an encoder. That is important as this is a finetuning architecture in which the encoder is replaced by the embeddings from the pre-trained Clay model.\nIn comparison, the network we are using to train the downstream task from scratch looks notably different:\nModel(\n  (encoder): Sequential(\n    (0): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): ConvTranspose2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Upsample(scale_factor=2.0, mode='nearest')\n    (5): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n)\nIn this architecture, there is a defined encoder since the embeddings aren’t doing the purpose of encoding latent information.\nFor both the finetuning and “from scratch” architectures, we use a binary_cross_entropy_with_logits loss function as this is a binary segmentation problem, and on the predictions, we run sigmoid and max functions to obtain final segmentation results.\nThe way we measure relative performance between the finetuned and “from scratch” model variants happens through calculation of evalution metrics common for segmentation, such as Dice coefficient, Intersection over Union, F1 score, precision and recall.\n\nLinear probing\nFor linear probing, we implement the finetuned architecture in a PyTorch callback that will execute every n epochs during the Foundation model’s training."
  },
  {
    "objectID": "inputdata.html",
    "href": "inputdata.html",
    "title": "Input data",
    "section": "",
    "text": "When running Clay with input data, you need to first prepare the data. Depending on the Clay model version, the model expects specific files, bands, and data types. This package provides utilities to prepare the data for Clay.\n\nsource\n\nfactory\n\n factory (locations:Union[shapely.geometry.point.Point,shapely.geometry.po\n          lygon.Polygon,List[Union[shapely.geometry.point.Point,shapely.ge\n          ometry.polygon.Polygon]]],\n          times:Union[datetime.time,List[datetime.time]],\n          model_version:str, local_folder:pathlib.Path)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nlocations\nUnion\nlocation point to prepare data.\n\n\ntimes\nUnion\nlist of times to prepare data.\n\n\nmodel_version\nstr\nmodel version for which to prepare data.\n\n\nlocal_folder\nPath\nlocal folder to store data.\n\n\n\nThe unit of data is the section of a MGRS tile of size 512x512, called “patch”. When giving a point, the data factory will create only the data for that patch.\n\ncopenhagen = Point(12.568337, 55.676098)\ntime = datetime.datetime(2020, 1, 1)\nmodel_version = \"v0\"\nlocal_folder = Path(\"tmp/data\")\n\ninput_data=factory(copenhagen,time,model_version,local_folder)\nprint(f\"{input_data.len} files.\")\n\nMethod prepare_data_for_location_and_time not implemented yet.\n3 files.\n\n\nYou can visualize the data with the data.rgb method.\n\ninput_data.rgb()\n\nMethod not implemented yet."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "You can manage the Clay Model with the Model class.\nsource"
  },
  {
    "objectID": "model.html#train-from-scratch",
    "href": "model.html#train-from-scratch",
    "title": "Model",
    "section": "Train from scratch",
    "text": "Train from scratch\nIf you want to train from scratch, you first must prepare the input data. Let’s take Copenhagen in Jan 1st, 2020 as an example.\n\ncopenhagen = Point(55.6761, 12.5683)\ndate = datetime.datetime(2020, 1, 1)\nlocal_folder = Path(\"data/\")\n\ninput_data = madewithclay.data.factory(copenhagen, date, model_version=model.version, local_folder=local_folder)\nprint(f\"creates {input_data.len} files\")\n\nMethod prepare_data_for_location_and_time not implemented yet.\ncreates 3 files\n\n\nTrain the model\npython trainer.py fit –ckpt_path=checkpoints/last.ckpt –data.data_dir=local_folder\nNow you can use the trained model to create embeddings for the input data.\npython trainer.py predict –ckpt_path=checkpoints/last.ckpt –data.data_dir=local_folder\nThis is a draft class. No more methods are implemented yet.\nSee model README for more details how to train and fine-tune the model."
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Embeddings in the context of Earth Observation (EO) and machine learning are dense, low-dimensional representations of high-dimensional data. In simple terms, they are numerical vectors that capture the essence of complex data, such as satellite imagery or temporal sequences from Earth observation instruments. These vectors are generated by models like Clay through a process of learning, where the model identifies and encodes the most important features and patterns within the data."
  },
  {
    "objectID": "embeddings.html#generating-embeddings",
    "href": "embeddings.html#generating-embeddings",
    "title": "Embeddings",
    "section": "Generating Embeddings",
    "text": "Generating Embeddings\nYou can use a Clay model to create new embeddings.\nYou will need to collect and pepare the required inputs. You can use clay.data.factory to download and prepare the data.\n\nlocation = Point(12.5, 55.6) # Copenhagen\ntime = datetime(2019,1,1)\nmodel_version = 0.0\nlocal_path = Path('tmp/data')\nmadewithclay.data.factory(location,time,model_version,local_path);\n\nMethod prepare_data_for_location_and_time not implemented yet.\n\n\n\nProducing embeddings from the pretrained model\nOnce you have the data prepared. Step by step instructions to create embeddings for a single MGRS tile location (e.g. 27WXN).\n\nEnsure that you can access the 13-band GeoTIFF data files.\naws s3 ls s3://clay-tiles-02/02/27WXN/\nThis should report a list of filepaths if you have the correct permissions, otherwise, please set up authentication before continuing.\nDownload the pretrained model weights, and put them in the checkpoints/ folder.\naws s3 cp s3://clay-model-ckpt/v0/clay-small-70MT-1100T-10E.ckpt checkpoints/\nFor running model inference on a large scale (hundreds or thousands of MGRS\ntiles), it is recommended to have a cloud VM instance with:\n\n1. A high bandwidth network (&gt;25Gbps) to speed up data transfer from the S3\n   bucket to the compute device.\n2. An NVIDIA Ampere generation GPU (e.g. A10G) or newer, which would allow\n   for efficient bfloat16 dtype calculations.\n\nFor example, an AWS g5.4xlarge instance would be a cost effective option.\n\nOnce you have a pretrained model, it is now possible to pass some input images into the encoder part of the Vision Transformer, and produce vector embeddings which contain a semantic representation of the image. 3. Run model inference to generate the embeddings.\npython trainer.py predict --ckpt_path=checkpoints/clay-small-70MT-1100T-10E.ckpt \\\n                          --trainer.precision=bf16-mixed \\\n                          --data.data_dir=s3://clay-tiles-02/02/27WXN \\\n                          --data.batch_size=32 \\\n                          --data.num_workers=16\nThis should output a GeoParquet file containing the embeddings for MGRS tile 27WXN (recall that each 10000x10000 pixel MGRS tile contains hundreds of smaller 512x512 chips), saved to the data/embeddings/ folder. See the next sub-section for details about the embeddings file.\nFor those interested in how the embeddings were computed, the predict step\nabove does the following:\n\n1. Pass the 13-band GeoTIFF input into the Vision Transformer's encoder, to\n   produce raw embeddings of shape (B, 1538, 768), where B is the batch_size,\n   1538 is the patch dimension and 768 is the embedding length. The patch\n   dimension itself is a concatenation of 1536 (6 band groups x 16x16\n   spatial patches of size 32x32 pixels each in a 512x512 image) + 2 (latlon\n   embedding and time embedding) = 1538.\n2. The mean or average is taken across the 1536 patch dimension, yielding an\n   output embedding of shape (B, 768).\n\nMore details of how this is implemented can be found by inspecting the\n`predict_step` method in the `model_clay.py` file.\n\n\nFormat of the embeddings file\nThe vector embeddings are stored in a single column within a GeoParquet file (*.gpq), with other columns containing spatiotemporal metadata. This file format is built on top of the popular Apache Parquet columnar storage format designed for fast analytics, and it is highly interoperable across different tools like QGIS, GeoPandas (Python), sfarrow (R), and more.\n\nFilename convention\nThe embeddings file utilizes the following naming convention:\n{MGRS:5}_{MINDATE:8}_{MAXDATE:8}_v{VERSION:3}.gpq\nExample: 27WXN_20200101_20231231_v001.gpq\n\n\n\nVariable\nDescription\n\n\n\n\nMGRS\nThe spatial location of the file’s contents in the Military Grid Reference System (MGRS), given as a 5-character string\n\n\nMINDATE\nThe minimum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format\n\n\nMINDATE\nThe maximum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format\n\n\nVERSION\nVersion of the generated embeddings, given as a 3-digit number\n\n\n\n\n\nTable schema\nEach row within the GeoParquet table is generated from a 512x512 pixel image, and contains a record of the embeddings, spatiotemporal metadata, and a link to the GeoTIFF file used as the source image for the embedding. The table looks something like this:\n\n\n\n\n\n\n\n\n\nsource_url\ndate\nembeddings\ngeometry\n\n\n\n\ns3://…/…/claytile_*.tif\n2021-01-01\n[0.1, 0.4, … x768]\nPOLYGON(…)\n\n\ns3://…/…/claytile_*.tif\n2021-06-30\n[0.2, 0.5, … x768]\nPOLYGON(…)\n\n\ns3://…/…/claytile_*.tif\n2021-12-31\n[0.3, 0.6, … x768]\nPOLYGON(…)\n\n\n\nDetails of each column are as follows:\n\nsource_url (string) - The full URL to the 13-band GeoTIFF image the embeddings were derived from.\ndate (date32) - Acquisition date of the Sentinel-2 image used to generate the embeddings, in YYYY-MM-DD format.\nembeddings (FixedShapeTensorArray) - The vector embeddings given as a 1-D tensor or list with a length of 768.\ngeometry (binary) - The spatial bounding box of where the 13-band image, provided in a WKB Polygon representation.\n\nAdditional technical details of the GeoParquet file:\n- GeoParquet specification [v1.0.0](https://geoparquet.org/releases/v1.0.0)\n- Coordinate reference system of geometries are in `OGC:CRS84`.\n\n\n\nEmbeddings Factory\nIf you don’t have embeddings, you’ll need to use the “Embeddings Factory”. It uses a given location and time, and a Clay model, to generate the embeddgins for each input data bundle."
  },
  {
    "objectID": "embeddings.html#working-with-embeddings",
    "href": "embeddings.html#working-with-embeddings",
    "title": "Embeddings",
    "section": "Working with embeddings",
    "text": "Working with embeddings\nA Clay embedding filename will look like this 33PWP_20181021_20200114_v001.gpq which is a concatenation of the following:\n\n33PWP - the location of the input data it comes from, in MGRS format.\n20181021 - the earliest date for any band of the input data it comes from\n20200114 - the latest date for any band of the input data it comes from\nv001 - the embedding version number.\n.gpq - the file extension, geoparquet.\n\nInside each file there will be as many rows as chips the MGRS tile was split into. as and each row will have a column for each of the embedding dimensions. The number of dimensions will depend on the Clay model used to generate the embeddings.\n\nsource\n\nEmbeddingsHandler\n\n EmbeddingsHandler (path:pathlib.Path, max_files:int=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nPath\n\nPath to the file or folder with files\n\n\nmax_files\nint\nNone\n\n\n\n\nEmbeddingsHandler has several methods to help you work with embeddings.\nThis is how you can load embeddings from a file or folder with files, including limiting the number of embeddings to load:\n\nsource\n\n\nEmbeddingsHandler.read_geoparquet_file\n\n EmbeddingsHandler.read_geoparquet_file (file:pathlib.Path)\n\nReads a geoparquet file and returns a dataframe with the embeddings.\n\n\n\n\nType\nDetails\n\n\n\n\nfile\nPath\nPath to the geoparquet file\n\n\n\nFor example, this is how to read up to 10 random files from a folder:\n\nembeddings_path = Path(\"../fixtures/sample_embedding/01WCP_20170701_20210603_v001.gpq\")\nembeddings = EmbeddingsHandler(embeddings_path, max_files=10)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:00&lt;00:00, 26.94it/s]\n\n\nTotal rows: 5\n Merging dataframes...\nDone!\n Total rows:  5\n\n\nThen you can plot the embeddings:\n\nsource\n\n\nEmbeddingsHandler.plot_locations\n\n EmbeddingsHandler.plot_locations\n                                   (figsize:[&lt;class'int'&gt;,&lt;class'int'&gt;]=(1\n                                   0, 10), alpha:float=0.2,\n                                   max_rows:int=10000,\n                                   bounds:List[int]=None,\n                                   indices:List[int]=None)\n\nPlots the dataframe on a map with an OSM underlay.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfigsize\n[&lt;class ‘int’&gt;, &lt;class ‘int’&gt;]\n(10, 10)\nSize of the plot\n\n\nalpha\nfloat\n0.2\nTransparency of the points\n\n\nmax_rows\nint\n10000\nRandom max number of rows to plot\n\n\nbounds\nList\nNone\nBounds of the plot [xmin, ymin, xmax, ymax]\n\n\nindices\nList\nNone\nIndices of the rows to plot\n\n\n\n\nembeddings.plot_locations()\n\n\n\n\n\nembeddings.plot_locations(indices=[0,1,2,3], max_rows=2)\n\n\n\n\nIf the total areas is too big, you can visualize the embeddings areas on detail zoomin in around one:\n\n# Get the coordinates of one geometry\nfirst_geometry = embeddings.gdf.loc[0].geometry\n# Create a 1km buffer around the first geometry\nbuffer = first_geometry.buffer(100 * 1000)  # 100 x 1km\n\nbounds = buffer.bounds\n\n# Call the plot method with the bounds\nembeddings.plot_locations(bounds=bounds)\n\n\n\n\nNote that we are using a transparency alpha=0.2. Different shades of darkness are locations where there are several embeddings stacked on top of each other, i.e. from different times.\nTo retrieve the RGB image for a given embedding, you can use the rgb_imgs method. the first time it will use the S3 url location to pull only the RGB bands, then save it locally for faster later retrieval.\nYou must specify the rows you want to retrieve, and if the first time, the output folder where to save the images, if it can’t reuse an existing local folder.\n\nlocal_path = Path(\"tmp/rgbs/\")\nlocal_path.mkdir(parents=True,exist_ok=True)\n\nembeddings.rgb_imgs(\n    [0,1,2], local_folder=Path(local_path)\n)\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]100%|██████████| 3/3 [00:02&lt;00:00,  1.02it/s]\n\n\n\n\n\nYou can skip the local_folder argument if you already have other local rgb saved.\n\nembeddings.rgb_imgs(2)\n\n100%|██████████| 1/1 [00:00&lt;00:00, 122.52it/s]\n\n\n\n\n\nIf needed you can force_fetch from the S3 location again.\n\nembeddings.rgb_imgs(0, force_fetch=True)\n\n100%|██████████| 1/1 [00:00&lt;00:00,  1.01it/s]"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html",
    "href": "Clay Model releases/clay v0 release.html",
    "title": "Clay Model v0 release",
    "section": "",
    "text": "This document outlines the v0 release of our deep learning model designed for Earth Observation (EO), leveraging Sentinel satellite data. The model aims to provide advanced insights and analysis for various EO applications.\n\n\nWe follow the Model Card Toolkit. It provides a summary of the model’s purpose and behavior, as well as information on the dataset it was trained on and the people and organizations involved in its creation. The model card is intended to help enable responsible use of AI systems by documenting their use cases and limitations.\n\n\n\nName: Clay Model\nVersion: v0.1\nType: Geospatial Visual Transformer trained with a MAE objective.\nTraining Data: ~1M spatiotemporal locations statistically sampled to contain a variety of land cover types. For each location, the bundle has: 10 Sentinel-2 bands, 2 Sentinel-1 bands and a DEM.\nDomain: Earth Observation\nAuthor: Clay.foundation\nContact: info@madewithclay.org\nRelease Date: 2023-12-31\nLicense: For the code, Apache 2.0. For the trained model OPEN RAIL-M\n\n\n\n\n\nThis model is intended as a foundational EO model. It can directly be used to create semantic embeddings, or be finetuned for classification or regression tasks. It can also be finetuned with other input data.\nIt is inteded to be used by non-profits, researchers, journalists, developers and data scientists. It is also intended to be used by commercial entities. Hence it is released with an open data and only trained with fully open data."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#model-card",
    "href": "Clay Model releases/clay v0 release.html#model-card",
    "title": "Clay Model v0 release",
    "section": "",
    "text": "We follow the Model Card Toolkit. It provides a summary of the model’s purpose and behavior, as well as information on the dataset it was trained on and the people and organizations involved in its creation. The model card is intended to help enable responsible use of AI systems by documenting their use cases and limitations.\n\n\n\nName: Clay Model\nVersion: v0.1\nType: Geospatial Visual Transformer trained with a MAE objective.\nTraining Data: ~1M spatiotemporal locations statistically sampled to contain a variety of land cover types. For each location, the bundle has: 10 Sentinel-2 bands, 2 Sentinel-1 bands and a DEM.\nDomain: Earth Observation\nAuthor: Clay.foundation\nContact: info@madewithclay.org\nRelease Date: 2023-12-31\nLicense: For the code, Apache 2.0. For the trained model OPEN RAIL-M"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#intended-use",
    "href": "Clay Model releases/clay v0 release.html#intended-use",
    "title": "Clay Model v0 release",
    "section": "",
    "text": "This model is intended as a foundational EO model. It can directly be used to create semantic embeddings, or be finetuned for classification or regression tasks. It can also be finetuned with other input data.\nIt is inteded to be used by non-profits, researchers, journalists, developers and data scientists. It is also intended to be used by commercial entities. Hence it is released with an open data and only trained with fully open data."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#training-information",
    "href": "Clay Model releases/clay v0 release.html#training-information",
    "title": "Clay Model v0 release",
    "section": "Training Information",
    "text": "Training Information\nThe training Objective is a Masked Autoencoder. We mask up to 70% of the input pixels across all bands (same location across bands) and train the model to predict the masked pixels.\n\nModel Hyperparameters:\n\nEffective Batch Size is 200. (Batch per GPU - 10, and 4 GPUs. Gradient accumulation every 5 batches, so 4* 10 * 5 =&gt; 200.\nLearning rate: Cosine Decay with Warm Restarts as our LR scheduler.\nOptimizer: Adam\nNumber of parameters: 127M\nModel filesize: ~500MB\nNumber of epochs: 20, for v0.1. 02 for v0.\nWe use the model setup clay_small:\n\nMAE mask ratio: 0.75,\nImage size: 512,\nSelf-attention patch size: 32,\nENCODER:\n\ndim: 768,\ndepth: 12,\nheads: 12,\ndim_head: 64,\nmlp_ratio: 4,\ndropout: 0.0,\nemb_dropout: 0.0,\n\nDECODER:\n\ndim: 512,\ndepth: 8,\nheads: 8,\ndim_head: 64,\nmlp_ratio: 4,\ndropout: 0.0,\nemb_dropout: 0.0\n\n\n\n\nWe do not use Data Augmentation. With learnable location and time embeddings we cannot use data augmentation. We do not use any other regularization techniques. We not use MixUp or CutMix, as it would need a complex tweak to carry over the metadata (location and time) to the mixed up images."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#performance-metrics",
    "href": "Clay Model releases/clay v0 release.html#performance-metrics",
    "title": "Clay Model v0 release",
    "section": "Performance Metrics",
    "text": "Performance Metrics\nThe model shows the following performance characteristics for its Masked Autoencoder objective: * Training loss: 0.52 for v0.1. * Validation loss: 0.46 for v0.1."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#known-limitations",
    "href": "Clay Model releases/clay v0 release.html#known-limitations",
    "title": "Clay Model v0 release",
    "section": "Known Limitations",
    "text": "Known Limitations\n\nThe model is trained on Sentinel data only.\nSentinel data only covers land and coastal waters.\nWe only train on a ver small sample of the Sentinel archives, both in terms of spatial coverage and time.\nWe do not train on the poles, and we do not train on open ocean, nor ocean nor atmospheric data.\nWe do not train on night time data.\nWe do not explicitly include extreme events in the training data.\nFor v0 we only train at most 3 different times per location.\n\n\nClouds and other source of “noise”:\nIn most EO applications clouds, cloud shadows, smog, atmospheric scattering, mid-air planes and other non-ground registrations are considered noise. We explicitly filter our clouds on our chips, but small clouds and their shadows might be present. As we increase the number of observations per location, and bands, we expect the model to learn to ignore single events but register patterns (places that are often cloudy or with smog)."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#known-biases",
    "href": "Clay Model releases/clay v0 release.html#known-biases",
    "title": "Clay Model v0 release",
    "section": "Known Biases:",
    "text": "Known Biases:\n[Discuss any biases]"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#ethical-considerations",
    "href": "Clay Model releases/clay v0 release.html#ethical-considerations",
    "title": "Clay Model v0 release",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nOur goal is to lower the barrier to use EO data for biodiversity and climate change mitigation and adaptation.\nWe focus on the undifferentiated baseline compute for most EO applications with a fully permissive license. This way we can encourage downstream users, both non-profit and for-profit to leapfrog their AI4EO services made with Clay, our independent, benchmarked, operational, open model. We aim thus to reducing the overall carbon footprint of EO applications, while providing state of the art baseline models for everyone."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#training-process",
    "href": "Clay Model releases/clay v0 release.html#training-process",
    "title": "Clay Model v0 release",
    "section": "Training Process",
    "text": "Training Process\n\n1. Data Factory\nWe use Microsoft Planetary Computer to pull all the data. Code\nFor v0 we used ESA’s Worldcover 2021 to create a statistically representative sample of the Earth’s land cover. Code.\nOur sampling function aims to select ~1000 MGRS tiles, with 200 samples from the 2000 most land-cover diverse tiles, 50 samples from the 1000 single lanc cover class for all categories except water ( urban, wetland, mangroves, moss, cropland, trees, shrubland, grassland, bare, snow). For coastal data we select 100 samples from all tiles with between 30% an 70% water.\nFor each selected MGRS tile, we split it into 512x512 chips. We filter out chips with more than 30% clouds or bad pixels.\nWe save each band into a different .tif file following the schema claytile_{mgrs}_{date}_v{version}_{counter}.tif where mgrs is the MGRS tile, date is the date of the observation, version is the version of the sampling strategy and counter is the counter of the chip in the tile [from left to right, from top to bottom].\nFinally we save that into S3 under the version of the sampling strategy folder.\nClay model v0 uses v2 sampling strategy, and corresponds to 1,045,224 chips (each a .tif file with 13 bands) and 6.4 TB.\nThe exact list of files used for v0 is here [5Mb zipped, 88Mb .txt list], generated using:\naws s3 ls s3://clay-tiles-02/ --recursive --human-readable --no-paginate --summarize &gt; s3_listing_data_v2_model_v0.txt\n\n\n2. Main Trainning loop\npython trainer.py fit     --ckpt_path=checkpoints/last.ckpt \\\n                          --data.batch_size=1024 \\  #change depending on your GPU memory\n                          --data.data_dir=s3://clay-tiles-02 \n\n\n3. Embeddings creation.\nWe create embeddings for all the training data.\nWe use the same training data source:\npython trainer.py predict --ckpt_path=checkpoints/last.ckpt \\\n                          --data.batch_size=1024 \\  \n                          --data.data_dir=s3://clay-tiles-02"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#computational-resources",
    "href": "Clay Model releases/clay v0 release.html#computational-resources",
    "title": "Clay Model v0 release",
    "section": "Computational Resources",
    "text": "Computational Resources\n\nCompute: Clay v0 and v0.1 are the same training run on on AWS g5.12xlarge instance, which has 4 A10G GPUs (24 GB VRAM in each). v0.1 is just more epochs.\nTraining Time: Each epoch takes ~15 hour. The model v0 was trained for 2 epochs, and v0.1 for 20 epochs. We did not continue as we saw both the training and validation losses plateauing."
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#embeddings-exploration",
    "href": "Clay Model releases/clay v0 release.html#embeddings-exploration",
    "title": "Clay Model v0 release",
    "section": "Embeddings Exploration",
    "text": "Embeddings Exploration\nThe Embeddings in our model are the output of the Transformer encoder. It’s the “semantic” bottleneck betwen the encoder and the decoder. The encoder needs to abstract the semantics of the image into a single vector, so the decoder can reconstruct the image from that vector.\nThese embeddings start as random vectors and slowly converge to a meaningful representation of the input data.\nLet’s read all the embeddings from the trainning data and see how they look like.\n\nembeddings_path = Path(\"../../data/clay-vector-embeddings-v001\")\nembeddings = madewithclay.embeddings.EmbeddingsHandler(embeddings_path)\n\n100%|██████████| 1088/1088 [00:04&lt;00:00, 224.62it/s]\n\n\nTotal rows: 947019\n Merging dataframes...\nDone!\n Total rows:  947019\n\n\n\nembeddings.gdf.head(n=1)\n\n\n\n\n\n\n\n\nsource_url\ndate\nembeddings\ngeometry\nx\ny\nlocation\nstart_date\nend_date\nversion\n\n\n\n\n0\ns3://clay-tiles-02/02/33PWP/2018-10-21/claytil...\n2018-10-21\n[0.0053703142, -0.0085538495, 0.0025720706, 0....\nPOLYGON ((1675014.721 1416156.064, 1675015.662...\n1.672391e+06\n1.418797e+06\n33PWP\n2018-10-21\n2020-01-14\nv001\n\n\n\n\n\n\n\n\nembeddings.plot_locations(max_rows=None)\n\n\n\n\nThese are some properties we expect from the embeddings:\n\nThe “length” (L2norm) and distribution of lengths is a one proxy for how much informationm and how much range of semantics they might contain.\nWe expect the embeddgins to cluster when representing different places with similar semantics. We can use tNSE to reduce the dimensionality of the 712 dimensions manyfold into 2D while trying to keep the clustering.\nIf we take visually similar chips, they should have a smaller distance between them than chips that are visually different. We can use cosine similarity to measure the distance between two embeddings.\n\n\nL2norm distribution\n\nembeddings.gdf[\"l2norm\"] = embeddings.gdf[\"embeddings\"].apply(\n    lambda x: np.linalg.norm(x, ord=2)\n)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].set_title(\"L2 norm histogram\")\nax[0].set_xlabel(\"L2 norm\")\nax[0].set_ylabel(\"Count\")\nax[0].hist(embeddings.gdf[\"l2norm\"], bins=100)\nax[1].set_title(\"L2 norm cumulative histogram\")\nax[1].set_xlabel(\"L2 norm\")\nax[1].set_ylabel(\"Cumulative count\")\nax[1].hist(embeddings.gdf[\"l2norm\"], bins=100, density=True, cumulative=True)\n\npeaks = [2.65, 2.83, 3.06]\nfor i in peaks:\n    ax[0].axvline(i, color=\"red\", linestyle=\"--\")\n    ax[0].axvline(i, color=\"red\", linestyle=\"--\")\n\n\n\n\nWe can see that ~80% of the embeddings are between 2.65 and 3.06, with two peaks on the edges. We can explore samples from the tails to see what is going on.\n\npeak = peaks[0]\nembeddings.gdf[\"distance_to_peak\"] = (embeddings.gdf[\"l2norm\"] - peak).abs()\n\n\ntop3 = embeddings.gdf.nsmallest(3, \"distance_to_peak\").index\n\n\nembeddings.rgb_imgs(top3,local_folder=Path('../../data/rgbs/'))\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]100%|██████████| 3/3 [00:04&lt;00:00,  1.40s/it]\n\n\n\n\n\nThey seem to correspond to chips with a lot of snow. I might make sense that the semantics here are both different and “smaller”.\nFor the second peak we thus expect more complex, yet rare, semantics. Since the lenght is bigger and the peak much smaller than the rest.\n\npeak = peaks[2]\nembeddings.gdf[\"distance_to_peak\"] = (embeddings.gdf[\"l2norm\"] - peak).abs()\n\ntop3 = embeddings.gdf.nsmallest(3, \"distance_to_peak\").index\nembeddings.rgb_imgs(top3)\n\n100%|██████████| 3/3 [00:07&lt;00:00,  2.53s/it]\n\n\n\n\n\nThis secondary peak seems to aggregate open water locations.\nAnd now, let’s just visualize the peak of the histogram, the most common “length” of the embeddings.\n\npeak = peaks[1]\nembeddings.gdf[\"distance_to_peak\"] = (embeddings.gdf[\"l2norm\"] - peak).abs()\n\ntop3 = embeddings.gdf.nsmallest(3, \"distance_to_peak\").index\nembeddings.rgb_imgs(top3)\n\n100%|██████████| 3/3 [00:03&lt;00:00,  1.29s/it]\n\n\n\n\n\nMin/Max entropy, as measure or “smooth” and “rough” semantics.\n\n#make the stdev of the vector on column embeddings\nembeddings.gdf[\"stdev\"] = embeddings.gdf[\"embeddings\"].apply(lambda x: np.std(x))\n#biggest stdev index\ntop3 = embeddings.gdf.nlargest(3, \"stdev\").index\nprint(top3)\nembeddings.rgb_imgs(top3,local_folder=Path('../../data/rgbs/'))\n\nIndex([391697, 466977, 646521], dtype='int64')\n\n\n100%|██████████| 3/3 [00:04&lt;00:00,  1.64s/it]\n\n\n\n\n\n\ntop3 = embeddings.gdf.nsmallest(3, \"stdev\").index\nprint(top3)\nembeddings.rgb_imgs(top3,local_folder=Path('../../data/rgbs/'))\n\nIndex([713536, 446810, 210232], dtype='int64')\n\n\n100%|██████████| 3/3 [00:08&lt;00:00,  2.82s/it]"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#benchmarking",
    "href": "Clay Model releases/clay v0 release.html#benchmarking",
    "title": "Clay Model v0 release",
    "section": "Benchmarking",
    "text": "Benchmarking"
  },
  {
    "objectID": "Clay Model releases/clay v0 release.html#fine-tuning",
    "href": "Clay Model releases/clay v0 release.html#fine-tuning",
    "title": "Clay Model v0 release",
    "section": "Fine-tuning",
    "text": "Fine-tuning\nWe can explore a fine-tuning experiment on the benchmark dataset. We’ll use the MGRS tile 33TTG.\nCreate the datacube for the tile, using the data factory:\n\nmadewithclay.data.factory('33TTG','2018-12-27','v0',Path('../../data/EU'));\n\nMethod prepare_data_for_location_and_time not implemented yet.\n\n\nWe can now take the pre-trained model and finetune it on the target dataset.\n\nmadewithclay.model.Model()\n\nOnly one version available for now: v0.0\nTarget model v0.0\n\n\n&lt;madewithclay.model.Model&gt;\n\n\npython trainer.py fit –ckpt_path=checkpoints/last.ckpt –data.data_dir=local_folder\npython trainer.py predict –ckpt_path=checkpoints/last.ckpt –data.data_dir=/home/brunosan/data/Clay/EU/\nWe can now load these fine-tuned embeddings\n\nEU_embeddings_path = Path(\"../../data/EU/embeddings/\")\nEU_embeddings = madewithclay.embeddings.EmbeddingsHandler(EU_embeddings_path)\nEU_embeddings.gdf.drop_duplicates(subset='geometry',inplace=True) #drop duplicates for this test\n\n100%|██████████| 2/2 [00:00&lt;00:00, 33.28it/s]\n\n\nTotal rows: 1097\n Merging dataframes...\nDone!\n Total rows:  1097\n\n\n\nEU_embeddings.plot_locations()\n\n\n\n\n\nEU_embeddings.gdf['chips'] = EU_embeddings.gdf.apply(lambda x: Path(x['source_url']).name, axis=1)\n\n\nembeddings.gdf['chips'] = embeddings.gdf.apply(lambda x: Path(x['source_url']).name, axis=1)\n\nOne way to compare the effect of finetunning is to compare the distance, and rgbs, of the closest embeddings from the pre-trained model and the fine-tuned model.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\nvector_list = np.stack(EU_embeddings.gdf.embeddings.values)\ndistance_matrix = squareform(pdist(vector_list, metric='cosine'))\n\n# Replace diagonal (self-distance) with np.nan to ignore them in finding the closest\nnp.fill_diagonal(distance_matrix, np.nan)\n\nclosest_indices_top_10 = np.empty((distance_matrix.shape[0], 10), dtype=int)\n\nfor i in range(distance_matrix.shape[0]):\n    closest_indices_top_10[i] = np.argpartition(distance_matrix[i], 10)[:10]\n\n# Display the result for the vectors with smallest distances\naverage_distances = []\n\n\nfor i, indices in enumerate(closest_indices_top_10):\n    # Compute the average distance of the top 10 closest vectors\n    avg_distance = np.mean(distance_matrix[i, indices])\n    average_distances.append(avg_distance)\n\n#add column with the closest indices and average distance\nEU_embeddings.gdf[\"closest_indices\"] = closest_indices_top_10.tolist()\nEU_embeddings.gdf[\"average_distance\"] = average_distances\n\n\n# histogram of average distances\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nax.set_title(\"Average distance to 10 closest vectors\")\nax.set_xlabel(\"Average distance\")\nax.set_xlim(0, .01)\nax.set_ylabel(\"Count\")\nax.axvline(0.0001, color=\"red\", linestyle=\"--\")\nax.hist(average_distances, bins=1000);\n\n\n\n\n\n#indices where average distance is less than 0.0001 are basically water\nwater = np.where(np.array(average_distances) &lt; 0.0001)[0]\n\n\nEU_embeddings.plot_locations(indices=water)\n\n\n\n\nThese are the 10 random locations with their semantically closest locations. First the reference, then the closest from left to right, from top to bottom.\n\nfor i in range(5):\n    row = EU_embeddings.gdf[EU_embeddings.gdf['average_distance'] &gt; 0.002 ].sample(1)\n    #print(row)\n    indices = [row.index[0],\n               *row['closest_indices'].values[0]]\n    print(indices)\n    EU_embeddings.plot_locations(indices=indices[:6])\n    EU_embeddings.rgb_imgs(indices[:6],local_folder=Path('../../data/rgbs/',verbose=True),force_fetch=True)\n\n[405, 125, 60, 275, 412, 130, 3, 416, 195, 333, 372]\n[181, 281, 70, 122, 175, 102, 264, 420, 286, 242, 210]\n[313, 110, 14, 171, 80, 69, 182, 209, 166, 251, 48]\n[74, 29, 236, 80, 137, 175, 181, 420, 251, 72, 410]\n[307, 192, 23, 295, 46, 276, 167, 433, 376, 88, 25]\n\n\n\n\n\n100%|██████████| 6/6 [00:00&lt;00:00, 189.97it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 376.79it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 239.45it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 172.84it/s]\n100%|██████████| 6/6 [00:00&lt;00:00, 185.18it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can check the distance to the closest embedding in the pre-trained model and the fine-tuned model. We can see that the fine-tuned model has understood finer details of the semantics of the locations, so the distances are bigger than the pre-trained model."
  }
]