{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp embeddings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "> Working with semantic embeddings of Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "import math\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import nbdev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import psycopg2\n",
    "import rasterio\n",
    "#from geoalchemy2 import Geometry\n",
    "from nbdev.showdoc import show_doc\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects.postgresql import ARRAY, DATE, FLOAT, VARCHAR\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Embeddings?\n",
    "\n",
    "Embeddings in the context of Earth Observation (EO) and machine learning are dense, low-dimensional representations of high-dimensional data. In simple terms, they are numerical vectors that capture the essence of complex data, such as satellite imagery or temporal sequences from Earth observation instruments. These vectors are generated by models like Clay through a process of learning, where the model identifies and encodes the most important features and patterns within the data.\n",
    "\n",
    "### Importance in EO\n",
    "- **Data Compression**: Embeddings condense the rich information present in satellite images into a more manageable form, facilitating easier storage and faster processing.\n",
    "- **Pattern Recognition**: They enable the model to recognize and compare patterns across large datasets, which is crucial for tasks like change detection, anomaly identification, or land cover classification.\n",
    "- **Semantic Interpretation**: Embeddings help in understanding the semantic content of EO data, such as differentiating between urban and forested areas, or recognizing the stages of crop growth.\n",
    "\n",
    "### How to Use Embeddings for EO\n",
    "\n",
    "1. **Feature Extraction**: Use Clay to process EO data and extract embeddings. These embeddings represent the key features of the data, capturing aspects like spectral signatures, texture, and temporal changes.\n",
    "\n",
    "2. **Similarity Searches**: Employ embeddings to perform similarity searches across EO datasets. For example, by comparing embeddings, you can find areas with similar land use patterns or detect regions showing similar changes over time.\n",
    "\n",
    "3. **Machine Learning Integration**: Embeddings can be used as input features for various machine learning models. In tasks like classification or regression, these embeddings provide a rich, pre-processed input that can significantly improve model performance.\n",
    "\n",
    "4. **Time-Series Analysis**: For temporal EO data, embeddings can capture the dynamics of changes over time, aiding in monitoring environmental changes, urban development, or agricultural practices.\n",
    "\n",
    "5. **Anomaly Detection**: Compare embeddings from different time periods or regions to identify anomalies or unexpected changes in the environment, such as sudden forest loss or unusual agricultural activity.\n",
    "\n",
    "In practice, to use embeddings in EO, you would typically process your EO dataset through the Clay model to generate embeddings, and then utilize these embeddings as per your specific application needs, be it for further analysis, integration into other models, or for direct comparisons and searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "\n",
    "Once you have a pretrained model, it is now possible to pass some input images\n",
    "into the encoder part of the Vision Transformer, and produce vector embeddings\n",
    "which contain a semantic representation of the image.\n",
    "\n",
    "### Producing embeddings from the pretrained model\n",
    "\n",
    "Step by step instructions to create embeddings for a single MGRS tile location\n",
    "(e.g. 27WXN).\n",
    "\n",
    "1. Ensure that you can access the 13-band GeoTIFF data files.\n",
    "\n",
    "   ```\n",
    "   aws s3 ls s3://clay-tiles-02/02/27WXN/\n",
    "   ```\n",
    "\n",
    "   This should report a list of filepaths if you have the correct permissions,\n",
    "   otherwise, please set up authentication before continuing.\n",
    "\n",
    "2. Download the pretrained model weights, and put them in the `checkpoints/`\n",
    "   folder.\n",
    "\n",
    "   ```bash\n",
    "   aws s3 cp s3://clay-model-ckpt/v0/clay-small-70MT-1100T-10E.ckpt checkpoints/\n",
    "   ```\n",
    "\n",
    "   ```{tip}\n",
    "   For running model inference on a large scale (hundreds or thousands of MGRS\n",
    "   tiles), it is recommended to have a cloud VM instance with:\n",
    "\n",
    "   1. A high bandwidth network (>25Gbps) to speed up data transfer from the S3\n",
    "      bucket to the compute device.\n",
    "   2. An NVIDIA Ampere generation GPU (e.g. A10G) or newer, which would allow\n",
    "      for efficient bfloat16 dtype calculations.\n",
    "\n",
    "   For example, an AWS g5.4xlarge instance would be a cost effective option.\n",
    "   ```\n",
    "\n",
    "3. Run model inference to generate the embeddings.\n",
    "\n",
    "   ```bash\n",
    "   python trainer.py predict --ckpt_path=checkpoints/clay-small-70MT-1100T-10E.ckpt \\\n",
    "                             --trainer.precision=bf16-mixed \\\n",
    "                             --data.data_dir=s3://clay-tiles-02/02/27WXN \\\n",
    "                             --data.batch_size=32 \\\n",
    "                             --data.num_workers=16\n",
    "   ```\n",
    "\n",
    "   This should output a GeoParquet file containing the embeddings for MGRS tile\n",
    "   27WXN (recall that each 10000x10000 pixel MGRS tile contains hundreds of\n",
    "   smaller 512x512 chips), saved to the `data/embeddings/` folder. See the next\n",
    "   sub-section for details about the embeddings file.\n",
    "\n",
    "   ```{note}\n",
    "   For those interested in how the embeddings were computed, the predict step\n",
    "   above does the following:\n",
    "\n",
    "   1. Pass the 13-band GeoTIFF input into the Vision Transformer's encoder, to\n",
    "      produce raw embeddings of shape (B, 1538, 768), where B is the batch_size,\n",
    "      1538 is the patch dimension and 768 is the embedding length. The patch\n",
    "      dimension itself is a concatenation of 1536 (6 band groups x 16x16\n",
    "      spatial patches of size 32x32 pixels each in a 512x512 image) + 2 (latlon\n",
    "      embedding and time embedding) = 1538.\n",
    "   2. The mean or average is taken across the 1536 patch dimension, yielding an\n",
    "      output embedding of shape (B, 768).\n",
    "\n",
    "   More details of how this is implemented can be found by inspecting the\n",
    "   `predict_step` method in the `model_clay.py` file.\n",
    "   ```\n",
    "\n",
    "\n",
    "### Format of the embeddings file\n",
    "\n",
    "The vector embeddings are stored in a single column within a\n",
    "[GeoParquet](https://geoparquet.org) file (*.gpq), with other columns\n",
    "containing spatiotemporal metadata. This file format is built on top of the\n",
    "popular Apache Parquet columnar storage format designed for fast analytics,\n",
    "and it is highly interoperable across different tools like QGIS,\n",
    "GeoPandas (Python), sfarrow (R), and more.\n",
    "\n",
    "#### Filename convention\n",
    "\n",
    "The embeddings file utilizes the following naming convention:\n",
    "\n",
    "```\n",
    "{MGRS:5}_{MINDATE:8}_{MAXDATE:8}_v{VERSION:3}.gpq\n",
    "```\n",
    "\n",
    "Example: `27WXN_20200101_20231231_v001.gpq`\n",
    "\n",
    "| Variable | Description |\n",
    "|--|--|\n",
    "| MGRS | The spatial location of the file's contents in the [Military Grid Reference System (MGRS)](https://en.wikipedia.org/wiki/Military_Grid_Reference_System), given as a 5-character string |\n",
    "| MINDATE | The minimum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format |\n",
    "| MINDATE | The maximum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format |\n",
    "| VERSION | Version of the generated embeddings, given as a 3-digit number |\n",
    "\n",
    "\n",
    "#### Table schema\n",
    "\n",
    "Each row within the GeoParquet table is generated from a 512x512 pixel image,\n",
    "and contains a record of the embeddings, spatiotemporal metadata, and a link to\n",
    "the GeoTIFF file used as the source image for the embedding. The table looks\n",
    "something like this:\n",
    "\n",
    "|         source_url          |    date    |      embeddings      |   geometry   |\n",
    "|-----------------------------|------------|----------------------|--------------|\n",
    "| s3://.../.../claytile_*.tif | 2021-01-01 | [0.1, 0.4, ... x768] | POLYGON(...) |\n",
    "| s3://.../.../claytile_*.tif | 2021-06-30 | [0.2, 0.5, ... x768] | POLYGON(...) |\n",
    "| s3://.../.../claytile_*.tif | 2021-12-31 | [0.3, 0.6, ... x768] | POLYGON(...) |\n",
    "\n",
    "Details of each column are as follows:\n",
    "\n",
    "- `source_url` ([string](https://arrow.apache.org/docs/python/generated/pyarrow.string.html)) - The full URL to the 13-band GeoTIFF image the embeddings were derived from.\n",
    "- `date` ([date32](https://arrow.apache.org/docs/python/generated/pyarrow.date32.html)) - Acquisition date of the Sentinel-2 image used to generate the embeddings, in YYYY-MM-DD format.\n",
    "- `embeddings` ([FixedShapeTensorArray](https://arrow.apache.org/docs/python/generated/pyarrow.FixedShapeTensorArray.html)) - The vector embeddings given as a 1-D tensor or list with a length of 768.\n",
    "- `geometry` ([binary](https://arrow.apache.org/docs/python/generated/pyarrow.binary.html)) - The spatial bounding box of where the 13-band image, provided in a [WKB](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry#Well-known_binary) Polygon representation.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Additional technical details of the GeoParquet file:\n",
    "- GeoParquet specification [v1.0.0](https://geoparquet.org/releases/v1.0.0)\n",
    "- Coordinate reference system of geometries are in `OGC:CRS84`.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Factory\n",
    "\n",
    "If you don't have embeddings, you'll need to use the \"Embeddings Factory\". It uses a given location and time, and a Clay model, to generate the embeddgins for each input data bundle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EmbeddingsFactory:\n",
    "    def __init__(self, model, output_directory):\n",
    "        \"\"\"\n",
    "        Initializes the Embeddings Factory with a model and an output directory.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "    def generate_embeddings(self, location_geojson, start_date, end_date, source):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a given location and time range.\n",
    "        \"\"\"\n",
    "\n",
    "    def _save_embeddings(self, embeddings, feature, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Saves the embeddings to a file or S3.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Clay embedding filename will look like this `33PWP_20181021_20200114_v001.gpq` which is a concatenation of the following:\n",
    "\n",
    "* `33PWP` - the location of the input data it comes from, in MGRS format.\n",
    "* `20181021` - the earliest date for any band of the input data it comes from\n",
    "* `20200114` - the latest date for any band of the input data it comes from\n",
    "* `v001` - the embedding version number.\n",
    "* `.gpq` - the file extension, geoparquet.\n",
    "\n",
    "Inside each file there will be as many rows as chips the MGRS tile was split into. as  and each row will have a column for each of the embedding dimensions. The number of dimensions will depend on the Clay model used to generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EmbeddingsHandler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Path,  # Path to the file or folder with files\n",
    "        max_files: int = None,\n",
    "    ):  # Max number of files to load, randomly chosen\n",
    "        self.path = Path(path)\n",
    "        self.gdf = None\n",
    "        self.files = None\n",
    "\n",
    "        # handle path\n",
    "        if self.path.is_dir():\n",
    "            self.files = list(self.path.glob(\"*.gpq\"))\n",
    "            if max_files is not None:\n",
    "                rng = np.random.default_rng()\n",
    "                self.files = rng.choice(self.files, size=max_files, replace=False)\n",
    "            assert len(self.files) > 0, \"No gpq files found in path\"\n",
    "        else:\n",
    "            self.files = [self.path]\n",
    "            assert self.path.suffix == \".gpq\", \"File must be a gpq file\"\n",
    "        self.load_geoparquet_folder()\n",
    "\n",
    "    def load_geoparquet_folder(\n",
    "        self,\n",
    "    ):\n",
    "        \"Load geoparquet files calling read_embeddings_file in parallel\"\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            gdfs = list(\n",
    "                tqdm(\n",
    "                    executor.map(self.read_geoparquet_file, self.files),\n",
    "                    total=len(self.files),\n",
    "                )\n",
    "            )\n",
    "        print(f\"Total rows: {sum([len(gdf) for gdf in gdfs])}\\n Merging dataframes...\")\n",
    "        gdf = pd.concat(gdfs, ignore_index=True)\n",
    "        gdf = gdf.drop('index', axis=1)\n",
    "        self.gdf = gdf\n",
    "        print(\"Done!\\n Total rows: \", len(self.gdf))\n",
    "\n",
    "    def read_geoparquet_file(self, file: Path):  # Path to the geoparquet file\n",
    "        \"\"\"\n",
    "        Reads a geoparquet file and returns a dataframe with the embeddings.\n",
    "        \"\"\"\n",
    "        assert file.exists(), \"Path does not exist\"\n",
    "        # check pattern of file name like 33PWP_20181021_20200114_v001.gpq\n",
    "        assert file.suffix == \".gpq\", \"File must be a gpq file\"\n",
    "        parts = file.stem.split(\"_\")\n",
    "        n_parts = len(\"33PWP_20181021_20200114_v001\".split(\"_\"))\n",
    "        assert len(parts) == n_parts, \"File name must have 4 parts\"\n",
    "        location, start_date, end_date, version = parts\n",
    "\n",
    "        # read file\n",
    "        gdf = gpd.read_parquet(file)\n",
    "        gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "        # add centroid x and y columns\n",
    "        gdf[\"x\"] = gdf.geometry.centroid.x\n",
    "        gdf[\"y\"] = gdf.geometry.centroid.y\n",
    "\n",
    "        # set columns for the values of location, start_date, end_date, version\n",
    "        gdf[\"location\"] = location\n",
    "        gdf[\"start_date\"] = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "        gdf[\"end_date\"] = datetime.strptime(end_date, \"%Y%m%d\")\n",
    "        gdf[\"version\"] = version\n",
    "        return gdf\n",
    "\n",
    "    def transform_crs(self, crs=\"epsg:3857\"):  # CRS to transform to\n",
    "        \"\"\"\n",
    "        Transforms the CRS of the dataframe.\n",
    "        \"\"\"\n",
    "        self.gdf = self.gdf.to_crs(crs)\n",
    "\n",
    "    def plot_locations(\n",
    "        self,\n",
    "        figsize: [int, int] = (10, 10),  # Size of the plot\n",
    "        alpha: float = 0.2,  # Transparency of the points\n",
    "        max_rows: int = 10000,  # Random max number of rows to plot\n",
    "        bounds: List[int] = None, # Bounds of the plot [xmin, ymin, xmax, ymax]\n",
    "        indices: List[int] = None # Indices of the rows to plot\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the dataframe on a map with an OSM underlay.\n",
    "        \"\"\"\n",
    "\n",
    "        # Default to all indices if none are provided\n",
    "        if indices is None:\n",
    "            indices = self.gdf.index.values\n",
    "\n",
    "        if max_rows is not None and len(indices) > max_rows:\n",
    "            self.gdf = self.gdf.drop_duplicates(subset=[\"geometry\"])\n",
    "            rng = np.random.default_rng()\n",
    "            indices = rng.choice(indices, size=max_rows, replace=False)\n",
    "        ax = self.gdf.loc[indices].plot(\n",
    "                figsize=figsize, alpha=alpha, edgecolor='k', markersize=1\n",
    "            )\n",
    "\n",
    "        # If bounds are provided, set the bounds of the plot\n",
    "        if bounds is not None:\n",
    "            ax.set_xlim(bounds[0], bounds[2])\n",
    "            ax.set_ylim(bounds[1], bounds[3])\n",
    "\n",
    "        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "        ax.set_axis_off()\n",
    "        plt.show()\n",
    "\n",
    "    def fetch_and_plot_image(\n",
    "        self,\n",
    "        index: int,  # index of the row to plot\n",
    "        local_folder: Path,  # Local folder to save the image\n",
    "        force_fetch: bool,  # Whether to force fetching the image\n",
    "        bands: List[int] = [3, 4, 2],\n",
    "    ):  # Bands to read\n",
    "        \"\"\"\n",
    "        Fetches an image from a URL or local path, reads RGB bands, and plots it.\n",
    "        \"\"\"\n",
    "        row = self.gdf.loc[index]\n",
    "\n",
    "        if row[\"local_path\"] is None:\n",
    "            force_fetch = True\n",
    "\n",
    "        if force_fetch:\n",
    "            # print(f\"Fetching image for row {index}\")\n",
    "            url = row[\"source_url\"]\n",
    "            local_path = local_folder / Path(url).name\n",
    "            assert local_folder.exists(), f\"Local folder {local_path} does not exist\"\n",
    "            with rasterio.open(url) as src:\n",
    "                # print(f\"Reading {bands} bands from {url}\")\n",
    "                rgb = src.read(bands)\n",
    "            with rasterio.open(\n",
    "                local_path,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=rgb.shape[1],\n",
    "                width=rgb.shape[2],\n",
    "                count=len(bands),\n",
    "                dtype=rgb.dtype,\n",
    "                crs=src.crs,\n",
    "                transform=src.transform,\n",
    "            ) as dst:\n",
    "                # print(f\"Writing {bands} bands to {local_path}\")\n",
    "                dst.write(rgb)\n",
    "                self.gdf.loc[self.gdf[\"source_url\"] == url, \"local_path\"] = str(local_path)\n",
    "        else:\n",
    "            #print(f\"Reading local image for row {index}\")\n",
    "            local_path = row[\"local_path\"]\n",
    "            with rasterio.open(local_path) as src:\n",
    "                # print(f\"Reading {bands} bands from {local_path}\")\n",
    "                rgb = src.read()\n",
    "        rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "        rgb = np.transpose(rgb, [1, 2, 0])\n",
    "\n",
    "        #clip values on each band to 10-90 percentile\n",
    "        percentiles = np.percentile(rgb, [10, 90], axis=(0, 1))\n",
    "        rgb = np.clip(rgb, percentiles[0], percentiles[1])\n",
    "        return rgb\n",
    "\n",
    "    def rgb_imgs(\n",
    "        self,\n",
    "        row_indices: Union[int, List[int]],  # Indices of the rows to plot\n",
    "        local_folder: Path = None,  # Local folder to save the image\n",
    "        force_fetch: bool = False,\n",
    "        skip_plot: bool = False # skip plotting, but save them to local_folder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots RGB images for specified rows,\n",
    "        either from local storage or by fetching them.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(row_indices, int):\n",
    "            row_indices = [row_indices]\n",
    "\n",
    "        if \"local_path\" not in self.gdf.columns:\n",
    "            self.gdf[\"local_path\"] = None\n",
    "\n",
    "        if force_fetch and local_folder is None:\n",
    "            if self.gdf[\"local_path\"].notnull().any():\n",
    "                existing_files = self.gdf[self.gdf[\"local_path\"].notnull()].iloc[0]\n",
    "                local_folder = Path(existing_files[\"local_path\"]).parent\n",
    "            else:\n",
    "                raise ValueError(\"local_folder must be provided if force_fetch is True\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "            results = list(tqdm(executor.map(\n",
    "                lambda idx: self.fetch_and_plot_image(idx,\n",
    "                                                      local_folder,\n",
    "                                                      force_fetch),\n",
    "                                                      row_indices),\n",
    "                                                      total=len(row_indices)))\n",
    "\n",
    "        if not skip_plot:\n",
    "            self._plot_images(results, row_indices)\n",
    "\n",
    "    def _plot_images(self,\n",
    "                     images  , # list of images\n",
    "                     indices): # list of indices\n",
    "        \"\"\"\n",
    "        Plots the images from the results of the fetch_and_plot_image method.\n",
    "        \"\"\"\n",
    "        num_images = len(images)\n",
    "        num_cols = min(3, num_images)\n",
    "        num_rows = -(-num_images // num_cols)  # Ceiling division\n",
    "\n",
    "        # Create a figure and a set of subplots\n",
    "        fig, axes = plt.subplots(nrows=num_rows,\n",
    "                                 ncols=num_cols,\n",
    "                                 figsize=(5 * num_cols, 5 * num_rows))\n",
    "        axes = axes.flatten() if num_images > 1 else [axes]\n",
    "\n",
    "        for idx, image in enumerate(images):\n",
    "            if image is not None:\n",
    "                ax = axes[idx]\n",
    "                ax.imshow(image)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"Index: {indices[idx]}\")\n",
    "\n",
    "        # Turn off axes for any unused subplots\n",
    "        for ax in axes[num_images:]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def prep_posgres(self,\n",
    "                     db_url: str): # Postgres database URL\n",
    "        \"\"\"\n",
    "        Prepares a Postgres database for the embeddings.\n",
    "        \"\"\"\n",
    "        self.db_url = db_url\n",
    "        self.engine = create_engine(db_url) if db_url else None\n",
    "\n",
    "    def save_to_postgres(self):\n",
    "        \"\"\"\n",
    "        Saves the geodataframe to PostgreSQL.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.db_url is None or self.engine is None:\n",
    "            raise ValueError(\"Database URL not provided or engine not initialized.\"+\n",
    "                             \"call .prep_posgres(db_url) first.\")\n",
    "\n",
    "        # Convert 'embeddings' from list to numpy array\n",
    "        self.gdf['embeddings'] = self.gdf['embeddings'].apply(lambda x: np.array(x))\n",
    "\n",
    "        # Define column types for PostgreSQL\n",
    "        column_types = {\n",
    "            'geometry': Geometry('POLYGON', srid=3857),\n",
    "            'embeddings': ARRAY(FLOAT),\n",
    "            'source_url': VARCHAR,\n",
    "            'local_url': VARCHAR,\n",
    "            'date': DATE,\n",
    "            'x': FLOAT,\n",
    "            'y': FLOAT,\n",
    "            'location': VARCHAR,\n",
    "            'start_date': DATE,\n",
    "            'end_date': DATE,\n",
    "            'version': VARCHAR\n",
    "        }\n",
    "\n",
    "        #check that all keys in column_types are in gdf.columns and print missing keys\n",
    "        missing_keys = set(column_types.keys()) - set(self.gdf.columns)\n",
    "        if len(missing_keys) > 0:\n",
    "            print(f\"Missing keys: {missing_keys}\")\n",
    "            raise ValueError(\"Missing keys in gdf.columns\")\n",
    "\n",
    "        self.gdf.to_sql('embeddings', self.engine, if_exists='replace', index=False, dtype=column_types)\n",
    "\n",
    "        # Save to PostgreSQL\n",
    "        self.gdf.to_sql('embeddings', self.engine, if_exists='replace', index=False, dtype=column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10000):\n",
    "    print(i)\n",
    "    embeddings_path = Path(\"/home/brunosan/data/Clay/clay-vector-embeddings-v001\")\n",
    "    embeddings = EmbeddingsHandler(embeddings_path, max_files=1000)\n",
    "\n",
    "    #make local_paths for all rows\n",
    "    embeddings.rgb_imgs(list(range(len(embeddings.gdf))),\n",
    "                        local_folder=Path(\"/home/brunosan/data/Clay/rgbs/\"),\n",
    "                        skip_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EmbeddingsHandler` has several methods to help you work with embeddings.\n",
    "\n",
    "This is how you can load embeddings from a file or folder with files, including limiting the number of embeddings to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EmbeddingsHandler.read_geoparquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is how to read 10 random files from a folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = Path(\"/home/brunosan/data/Clay/clay-vector-embeddings-v001\")\n",
    "embeddings = EmbeddingsHandler(embeddings_path, max_files=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can plot the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EmbeddingsHandler.plot_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.plot_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.plot_locations(indices=[0,1,2,3,4,5], max_rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total areas is too big, you can visualize the embeddings areas on detail zoomin in around one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of one geometry\n",
    "first_geometry = embeddings.gdf.loc[50].geometry\n",
    "# Create a 1km buffer around the first geometry\n",
    "buffer = first_geometry.buffer(100 * 1000)  # 100 x 1km\n",
    "\n",
    "bounds = buffer.bounds\n",
    "\n",
    "# Call the plot method with the bounds\n",
    "embeddings.plot_locations(bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using a transparency `alpha=0.2`. Darker areas are where there are several embeddings stacked on top of each other, from different times. \n",
    "\n",
    "We can plot the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the time range\n",
    "embeddings.gdf.start_date.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the RGB image for a given embedding, you can use the `rgb_imgs` method. the first time it will use the `S3` url location to pull only the RGB bands, then save it locally for faster later retrieval.\n",
    "\n",
    "You must specify the rows you want to retrieve, and if the first time, the output folder where to save the images, if it can't reuse an existing local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.rgb_imgs(\n",
    "    [0,1,2], local_folder=Path(\"/home/brunosan/data/Clay/rgbs/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the `local_folder` argument if you already have other local rgb saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.rgb_imgs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed you can `force_fetch` from the `S3` location again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.rgb_imgs(0, force_fetch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
