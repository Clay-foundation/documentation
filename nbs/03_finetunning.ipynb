{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp finetuning\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "> How to leverage a pretrained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning refers to a process in machine learning where a pre-trained model is further trained on a specific dataset or task. Clay is designed as a pre-trained model, utilizing a vast collection of historical data across various locations and times on Earth. It employs general self-supervised tasks, focusing on the undifferentiated heavy lifting useful across various fine-tuning tasks. E.g. shapes, edges, colors, patterns, ... Model starts from random values, so these basic features are very likely to be useful for many tasks.\n",
    "\n",
    "Fine-tunning is thus:\n",
    "* **Faster**: The model starts with many learned features and only needs to learn the nuances of the new data.\n",
    "* **More accurate**: The model, having been trained on a large dataset, has learned general features that might be difficult to acquire from a smaller, specific dataset.\n",
    "* **Resource efficient**: Users don't need to start from scratch and train a model from the ground up. Instead, they can use a pre-trained model and resume training on their specific needs. Thus requiring less computational power, less energy consumption, reduced carbon emissions, and cost savings. \n",
    "* **Strategic**: Clay pre-trained models create a common baseline, available to anyone. This public asset lets you focus directly on creating value for the specific location, times and tasks that matter to you.\n",
    "\n",
    "## Types of Fine-Tuning with Clay\n",
    "\n",
    "1. **Transfer Learning**: This involves taking a pre-trained Clay model and continuing its training with a specific dataset. It's the simplest form of fine-tuning, especially useful for tasks like extracting model embeddings or maintaining the same autoencoder task.\n",
    "\n",
    "2. **Full Fine-Tuning**: Here, a pre-trained Clay model is further trained with the same or different data for a different task (like a classification task). Often, the weights of the early layers are retained or \"frozen,\" and new layers are added to the model's end. Freezing early layers prevents \"catastrophic forgetting\", where the model overwrites basic semantics instead of building upon them.\n",
    "\n",
    "3. **Linear Probing**: In this method, the pre-trained model's weights are completely frozen, and only a linear classifier is trained on top of these features. This is a very fast technique, given that it only train one final layer. It is also somewhat limited task that don't need new complex semantics. This technique is actually used while training the Clay model, as a way to check the quality of the features learned by the model, as it learns.\n",
    "\n",
    "4. **LoRA (Low-Rank Adaptation)**: LoRA is similar to linear probing, in which all the model is kept frozen. Instead of training an added linear classifier at the output end of the model, LoRA inserts small trainable matrices inside the model. While finetuned there are only a few parameters to train so it is much faster that full fine-tune, but it also effectively changes the internal semantics. It is thus in between linear probing and full fine-tuning, in terms of speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's take a look at how we are finetuning on the benchmark datacube-adapted\n",
    "[Cloud to Street - Microsoft Flood Dataset](https://beta.source.coop/repositories/c2sms/c2smsfloods).\n",
    "As a reminder, that is a downstream\n",
    "segmentation task for identifiying water pixels in recorded flood events. It's\n",
    "a binary segmentation problem, specifically.\n",
    "\n",
    "We process the datacubes into batches formatted in the way the pretrained Clay\n",
    "model expects, with the addition of information for label images as well.\n",
    "Here's an example subset of a batch dictionary:\n",
    "\n",
    "```\n",
    "{'labels': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "           ...,\n",
    "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
    " 'pixels': tensor([[[[-0.5994, -0.6108, -0.6034,  ..., -0.5610, -0.5590, -0.5614],\n",
    "           [-0.5767, -0.5950, -0.6004,  ..., -0.5619, -0.5536, -0.5610],\n",
    "           [-0.5841, -0.5762, -0.5930,  ..., -0.5491, -0.5304, -0.5373],\n",
    "           ...,\n",
    "           [-0.5087, -0.5447, -0.4351,  ..., -0.6162, -0.6083, -0.6044],\n",
    "           [-0.4184, -0.5432, -0.5003,  ..., -0.6108, -0.6128, -0.6073],\n",
    "           [-0.2496, -0.5348, -0.5225,  ..., -0.6137, -0.6167, -0.6128]],\n",
    "\n",
    "          [[-0.6371, -0.6435, -0.6425,  ..., -0.5834, -0.5898, -0.5923],\n",
    "           [-0.6296, -0.6410, -0.6385,  ..., -0.5794, -0.5983, -0.5958],\n",
    "           [-0.6167, -0.6177, -0.6182,  ..., -0.5545, -0.5913, -0.5834],\n",
    "           ...,\n",
    "           [-0.4800, -0.5153, -0.4308,  ..., -0.6525, -0.6410, -0.6331],\n",
    "           [-0.4104, -0.5034, -0.4318,  ..., -0.6331, -0.6226, -0.6087],\n",
    "           [-0.2404, -0.5222, -0.4522,  ..., -0.6231, -0.6241, -0.6177]],\n",
    "\n",
    "          [[-0.7068, -0.7217, -0.7101,  ..., -0.6118, -0.6178, -0.6290],\n",
    "           [-0.7087, -0.7022, -0.6924,  ..., -0.6141, -0.6146, -0.6234],\n",
    "           [-0.7017, -0.6998, -0.6831,  ..., -0.5927, -0.6085, -0.6104],\n",
    "           ...,\n",
    "           [-0.5563, -0.5480, -0.4571,  ..., -0.7106, -0.7045, -0.6933],\n",
    "           [-0.4725, -0.5526, -0.4781,  ..., -0.6975, -0.6789, -0.6807],\n",
    "           [-0.3117, -0.4995, -0.5000,  ..., -0.6952, -0.6835, -0.6845]],\n",
    "\n",
    "          ...,\n",
    "          ]),\n",
    " 'bbox': tensor([[ 661415., 5369305.,  666535., 5374425.]], dtype=torch.float64),\n",
    " 'epsg': tensor([32633], dtype=torch.int32),\n",
    " 'date': ['2020-10-20'],\n",
    " 'latlon': tensor([[-0.8192, -0.7854]]),\n",
    " 'timestep': tensor([[-1.2217,  2.7132, -2.4086]]),\n",
    " 'source_url': ['S2A_L2A_20201022T100051_N0209_R122_T33UXP_20201022T111023_06144-02560_S1B_IW_GRDH_1SDV_20201020T164222_20201020T164247_023899_02D6C4_rtc']}\n",
    "```\n",
    "\n",
    "Batches of dictionaries like this run through the Clay model's encoder to\n",
    "generate embeddings, such as this:\n",
    "\n",
    "![embedding_ex](https://github.com/Clay-foundation/model/assets/23487320/375c9e83-d539-4730-b923-3b0b61ea689c)\n",
    "\n",
    "from batches with image bands such as:\n",
    "\n",
    "![band_red_ex](https://github.com/Clay-foundation/model/assets/23487320/0c254dbf-9589-4fbf-ab32-e3774fbd2f1a)\n",
    "\n",
    "and labels:\n",
    "\n",
    "![labels_ex](https://github.com/Clay-foundation/model/assets/23487320/a92eb8e7-9268-46e5-a254-132205cbc498)\n",
    "\n",
    "These embeddings are reshaped from shape\n",
    "`batch size * (band groups length * number of patches) * embedding size` to\n",
    "`batch size * (band groups length * embedding size) * patch height * patch width`\n",
    "before being passed to a series of 2D convolutional transpose and ReLU layers\n",
    "in a downstream decoder network.\n",
    "\n",
    "That decoder network is the core of the downstream task. In a forward pass, it\n",
    "ingests the embeddings, runs them through those layers and computes a loss\n",
    "value with respect to the labels. The loss is back-propagated and the decoder\n",
    "gradually finetunes itself to the downstream dataset. Here's a peek at the\n",
    "decoder layers:\n",
    "\n",
    "```\n",
    "Model(\n",
    "  (decoder): Sequential(\n",
    "    (0): Conv2d(4608, 64, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
    "    (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (3): ReLU(inplace=True)\n",
    "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
    "    (5): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (6): ReLU(inplace=True)\n",
    "    (7): Upsample(scale_factor=2.0, mode='nearest')\n",
    "    (8): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (9): ReLU(inplace=True)\n",
    "    (10): Upsample(scale_factor=2.0, mode='nearest')\n",
    "    (11): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (12): Upsample(scale_factor=2.0, mode='nearest')\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "Note the absence of an encoder. That is important as this is a finetuning\n",
    "architecture in which the encoder is replaced by the embeddings from the\n",
    "pre-trained Clay model.\n",
    "\n",
    "In comparison, the network we are using to train the downstream task from\n",
    "scratch looks notably different:\n",
    "\n",
    "```\n",
    "Model(\n",
    "  (encoder): Sequential(\n",
    "    (0): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (3): ReLU(inplace=True)\n",
    "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): ConvTranspose2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (3): ReLU(inplace=True)\n",
    "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
    "    (5): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  )\n",
    ")\n",
    "```\n",
    "In this architecture, there is a defined encoder since the embeddings aren't\n",
    "doing the purpose of encoding latent information.\n",
    "\n",
    "For both the finetuning and \"from scratch\" architectures, we use a\n",
    "`binary_cross_entropy_with_logits` loss function as this is a binary\n",
    "segmentation problem, and on the predictions, we run sigmoid and max functions\n",
    "to obtain final segmentation results.\n",
    "\n",
    "The way we measure relative performance between the finetuned and\n",
    "\"from scratch\" model variants happens through calculation of evalution metrics\n",
    "common for segmentation, such as Dice coefficient, Intersection over Union, F1\n",
    "score, precision and recall.\n",
    "\n",
    "### Linear probing\n",
    "\n",
    "For linear probing, we implement the finetuned architecture in a\n",
    "[PyTorch callback](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)\n",
    "that will execute every `n` epochs during the Foundation model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
