{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp embeddings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import madewithclay.data\n",
    "import madewithclay.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "> Working with semantic embeddings of Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import nbdev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import rasterio\n",
    "from geoalchemy2 import Geometry\n",
    "from nbdev.showdoc import show_doc\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects.postgresql import ARRAY, DATE, FLOAT, VARCHAR\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Embeddings?\n",
    "\n",
    "Embeddings in the context of Earth Observation (EO) and machine learning are dense, low-dimensional representations of high-dimensional data. In simple terms, they are numerical vectors that capture the essence of complex data, such as satellite imagery or temporal sequences from Earth observation instruments. These vectors are generated by models like Clay through a process of learning, where the model identifies and encodes the most important features and patterns within the data.\n",
    "\n",
    "### Importance in EO\n",
    "- **Data Compression**: Embeddings condense the rich information present in satellite images into a more manageable form, facilitating easier storage and faster processing.\n",
    "- **Pattern Recognition**: They enable the model to recognize and compare patterns across large datasets, which is crucial for tasks like change detection, anomaly identification, or land cover classification.\n",
    "- **Semantic Interpretation**: Embeddings help in understanding the semantic content of EO data, such as differentiating between urban and forested areas, or recognizing the stages of crop growth.\n",
    "\n",
    "### How to Use Embeddings for EO\n",
    "\n",
    "1. **Feature Extraction**: Use Clay to process EO data and extract embeddings. These embeddings represent the key features of the data, capturing aspects like spectral signatures, texture, and temporal changes.\n",
    "\n",
    "2. **Similarity Searches**: Employ embeddings to perform similarity searches across EO datasets. For example, by comparing embeddings, you can find areas with similar land use patterns or detect regions showing similar changes over time.\n",
    "\n",
    "3. **Machine Learning Integration**: Embeddings can be used as input features for various machine learning models. In tasks like classification or regression, these embeddings provide a rich, pre-processed input that can significantly improve model performance.\n",
    "\n",
    "4. **Time-Series Analysis**: For temporal EO data, embeddings can capture the dynamics of changes over time, aiding in monitoring environmental changes, urban development, or agricultural practices.\n",
    "\n",
    "5. **Anomaly Detection**: Compare embeddings from different time periods or regions to identify anomalies or unexpected changes in the environment, such as sudden forest loss or unusual agricultural activity.\n",
    "\n",
    "In practice, to use embeddings in EO, you would typically process your EO dataset through the Clay model to generate embeddings, and then utilize these embeddings as per your specific application needs, be it for further analysis, integration into other models, or for direct comparisons and searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "\n",
    "You can use a Clay model to create new embeddings. \n",
    "\n",
    "You will need to collect and pepare the required inputs. You can use `clay.data.factory` to download and prepare the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method not implemented yet.\n"
     ]
    }
   ],
   "source": [
    "location = Point(12.5, 55.6) # Copenhagen\n",
    "time = datetime(2019,1,1)\n",
    "model_version = 0.0\n",
    "local_path = Path('tmp/data')\n",
    "madewithclay.data.factory(location,time,model_version,local_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing embeddings from the pretrained model\n",
    "\n",
    "Once you have the data prepared.\n",
    "Step by step instructions to create embeddings for a single MGRS tile location\n",
    "(e.g. 27WXN).\n",
    "\n",
    "1. Ensure that you can access the 13-band GeoTIFF data files.\n",
    "\n",
    "   ```\n",
    "   aws s3 ls s3://clay-tiles-02/02/27WXN/\n",
    "   ```\n",
    "\n",
    "   This should report a list of filepaths if you have the correct permissions,\n",
    "   otherwise, please set up authentication before continuing.\n",
    "\n",
    "2. Download the pretrained model weights, and put them in the `checkpoints/`\n",
    "   folder.\n",
    "\n",
    "   ```bash\n",
    "   aws s3 cp s3://clay-model-ckpt/v0/clay-small-70MT-1100T-10E.ckpt checkpoints/\n",
    "   ```\n",
    "\n",
    "   ```{tip}\n",
    "   For running model inference on a large scale (hundreds or thousands of MGRS\n",
    "   tiles), it is recommended to have a cloud VM instance with:\n",
    "\n",
    "   1. A high bandwidth network (>25Gbps) to speed up data transfer from the S3\n",
    "      bucket to the compute device.\n",
    "   2. An NVIDIA Ampere generation GPU (e.g. A10G) or newer, which would allow\n",
    "      for efficient bfloat16 dtype calculations.\n",
    "\n",
    "   For example, an AWS g5.4xlarge instance would be a cost effective option.\n",
    "   ```\n",
    "\n",
    "\n",
    "Once you have a pretrained model, it is now possible to pass some input images\n",
    "into the encoder part of the Vision Transformer, and produce vector embeddings\n",
    "which contain a semantic representation of the image.\n",
    "3. Run model inference to generate the embeddings.\n",
    "\n",
    "   ```bash\n",
    "   python trainer.py predict --ckpt_path=checkpoints/clay-small-70MT-1100T-10E.ckpt \\\n",
    "                             --trainer.precision=bf16-mixed \\\n",
    "                             --data.data_dir=s3://clay-tiles-02/02/27WXN \\\n",
    "                             --data.batch_size=32 \\\n",
    "                             --data.num_workers=16\n",
    "   ```\n",
    "\n",
    "   This should output a GeoParquet file containing the embeddings for MGRS tile\n",
    "   27WXN (recall that each 10000x10000 pixel MGRS tile contains hundreds of\n",
    "   smaller 512x512 chips), saved to the `data/embeddings/` folder. See the next\n",
    "   sub-section for details about the embeddings file.\n",
    "\n",
    "   ```{note}\n",
    "   For those interested in how the embeddings were computed, the predict step\n",
    "   above does the following:\n",
    "\n",
    "   1. Pass the 13-band GeoTIFF input into the Vision Transformer's encoder, to\n",
    "      produce raw embeddings of shape (B, 1538, 768), where B is the batch_size,\n",
    "      1538 is the patch dimension and 768 is the embedding length. The patch\n",
    "      dimension itself is a concatenation of 1536 (6 band groups x 16x16\n",
    "      spatial patches of size 32x32 pixels each in a 512x512 image) + 2 (latlon\n",
    "      embedding and time embedding) = 1538.\n",
    "   2. The mean or average is taken across the 1536 patch dimension, yielding an\n",
    "      output embedding of shape (B, 768).\n",
    "\n",
    "   More details of how this is implemented can be found by inspecting the\n",
    "   `predict_step` method in the `model_clay.py` file.\n",
    "   ```\n",
    "\n",
    "\n",
    "### Format of the embeddings file\n",
    "\n",
    "The vector embeddings are stored in a single column within a\n",
    "[GeoParquet](https://geoparquet.org) file (*.gpq), with other columns\n",
    "containing spatiotemporal metadata. This file format is built on top of the\n",
    "popular Apache Parquet columnar storage format designed for fast analytics,\n",
    "and it is highly interoperable across different tools like QGIS,\n",
    "GeoPandas (Python), sfarrow (R), and more.\n",
    "\n",
    "#### Filename convention\n",
    "\n",
    "The embeddings file utilizes the following naming convention:\n",
    "\n",
    "```\n",
    "{MGRS:5}_{MINDATE:8}_{MAXDATE:8}_v{VERSION:3}.gpq\n",
    "```\n",
    "\n",
    "Example: `27WXN_20200101_20231231_v001.gpq`\n",
    "\n",
    "| Variable | Description |\n",
    "|--|--|\n",
    "| MGRS | The spatial location of the file's contents in the [Military Grid Reference System (MGRS)](https://en.wikipedia.org/wiki/Military_Grid_Reference_System), given as a 5-character string |\n",
    "| MINDATE | The minimum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format |\n",
    "| MINDATE | The maximum acquisition date of the Sentinel-2 images used to generate the embeddings, given in YYYYMMDD format |\n",
    "| VERSION | Version of the generated embeddings, given as a 3-digit number |\n",
    "\n",
    "\n",
    "#### Table schema\n",
    "\n",
    "Each row within the GeoParquet table is generated from a 512x512 pixel image,\n",
    "and contains a record of the embeddings, spatiotemporal metadata, and a link to\n",
    "the GeoTIFF file used as the source image for the embedding. The table looks\n",
    "something like this:\n",
    "\n",
    "|         source_url          |    date    |      embeddings      |   geometry   |\n",
    "|-----------------------------|------------|----------------------|--------------|\n",
    "| s3://.../.../claytile_*.tif | 2021-01-01 | [0.1, 0.4, ... x768] | POLYGON(...) |\n",
    "| s3://.../.../claytile_*.tif | 2021-06-30 | [0.2, 0.5, ... x768] | POLYGON(...) |\n",
    "| s3://.../.../claytile_*.tif | 2021-12-31 | [0.3, 0.6, ... x768] | POLYGON(...) |\n",
    "\n",
    "Details of each column are as follows:\n",
    "\n",
    "- `source_url` ([string](https://arrow.apache.org/docs/python/generated/pyarrow.string.html)) - The full URL to the 13-band GeoTIFF image the embeddings were derived from.\n",
    "- `date` ([date32](https://arrow.apache.org/docs/python/generated/pyarrow.date32.html)) - Acquisition date of the Sentinel-2 image used to generate the embeddings, in YYYY-MM-DD format.\n",
    "- `embeddings` ([FixedShapeTensorArray](https://arrow.apache.org/docs/python/generated/pyarrow.FixedShapeTensorArray.html)) - The vector embeddings given as a 1-D tensor or list with a length of 768.\n",
    "- `geometry` ([binary](https://arrow.apache.org/docs/python/generated/pyarrow.binary.html)) - The spatial bounding box of where the 13-band image, provided in a [WKB](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry#Well-known_binary) Polygon representation.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Additional technical details of the GeoParquet file:\n",
    "- GeoParquet specification [v1.0.0](https://geoparquet.org/releases/v1.0.0)\n",
    "- Coordinate reference system of geometries are in `OGC:CRS84`.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Factory\n",
    "\n",
    "If you don't have embeddings, you'll need to use the \"Embeddings Factory\". It uses a given location and time, and a Clay model, to generate the embeddgins for each input data bundle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Clay embedding filename will look like this `33PWP_20181021_20200114_v001.gpq` which is a concatenation of the following:\n",
    "\n",
    "* `33PWP` - the location of the input data it comes from, in MGRS format.\n",
    "* `20181021` - the earliest date for any band of the input data it comes from\n",
    "* `20200114` - the latest date for any band of the input data it comes from\n",
    "* `v001` - the embedding version number.\n",
    "* `.gpq` - the file extension, geoparquet.\n",
    "\n",
    "Inside each file there will be as many rows as chips the MGRS tile was split into. as  and each row will have a column for each of the embedding dimensions. The number of dimensions will depend on the Clay model used to generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EmbeddingsHandler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Path,  # Path to the file or folder with files\n",
    "        max_files: int = None,\n",
    "    ):  # Max number of files to load, randomly chosen\n",
    "        self.path = Path(path)\n",
    "        self.gdf = None\n",
    "        self.files = None\n",
    "\n",
    "        # handle path\n",
    "        if self.path.is_dir():\n",
    "            self.files = list(self.path.glob(\"*.gpq\"))\n",
    "            if max_files is not None:\n",
    "                rng = np.random.default_rng()\n",
    "                self.files = rng.choice(self.files, size=max_files, replace=False)\n",
    "            assert len(self.files) > 0, \"No gpq files found in path\"\n",
    "        else:\n",
    "            self.files = [self.path]\n",
    "            assert self.path.suffix == \".gpq\", \"File must be a gpq file\"\n",
    "        self.load_geoparquet_folder()\n",
    "\n",
    "    def load_geoparquet_folder(\n",
    "        self,\n",
    "    ):\n",
    "        \"Load geoparquet files calling read_embeddings_file in parallel\"\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            gdfs = list(\n",
    "                tqdm(\n",
    "                    executor.map(self.read_geoparquet_file, self.files),\n",
    "                    total=len(self.files),\n",
    "                )\n",
    "            )\n",
    "        print(f\"Total rows: {sum([len(gdf) for gdf in gdfs])}\\n Merging dataframes...\")\n",
    "        gdf = pd.concat(gdfs, ignore_index=True)\n",
    "        gdf = gdf.drop('index', axis=1)\n",
    "        self.gdf = gdf\n",
    "        print(\"Done!\\n Total rows: \", len(self.gdf))\n",
    "\n",
    "    def read_geoparquet_file(self, \n",
    "                             file: Path):  # Path to the geoparquet file\n",
    "        \"\"\"\n",
    "        Reads a geoparquet file and returns a dataframe with the embeddings.\n",
    "        \"\"\"\n",
    "        assert file.exists(), \"Path does not exist\"\n",
    "        # check pattern of file name like 33PWP_20181021_20200114_v001.gpq\n",
    "        assert file.suffix == \".gpq\", \"File must be a gpq file\"\n",
    "        parts = file.stem.split(\"_\")\n",
    "        n_parts = len(\"33PWP_20181021_20200114_v001\".split(\"_\"))\n",
    "        assert len(parts) == n_parts, \"File name must have 4 parts\"\n",
    "        location, start_date, end_date, version = parts\n",
    "\n",
    "        # read file\n",
    "        gdf = gpd.read_parquet(file)\n",
    "        gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "        # add centroid x and y columns\n",
    "        gdf[\"x\"] = gdf.geometry.centroid.x\n",
    "        gdf[\"y\"] = gdf.geometry.centroid.y\n",
    "\n",
    "        # set columns for the values of location, start_date, end_date, version\n",
    "        gdf[\"location\"] = location\n",
    "        gdf[\"start_date\"] = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "        gdf[\"end_date\"] = datetime.strptime(end_date, \"%Y%m%d\")\n",
    "        gdf[\"version\"] = version\n",
    "        return gdf\n",
    "\n",
    "    def transform_crs(self, crs=\"epsg:3857\"):  # CRS to transform to\n",
    "        \"\"\"\n",
    "        Transforms the CRS of the dataframe.\n",
    "        \"\"\"\n",
    "        self.gdf = self.gdf.to_crs(crs)\n",
    "\n",
    "    def plot_locations(\n",
    "        self,\n",
    "        figsize: [int, int] = (10, 10),  # Size of the plot\n",
    "        alpha: float = 0.2,  # Transparency of the points\n",
    "        max_rows: int = 10000,  # Random max number of rows to plot\n",
    "        bounds: List[int] = None, # Bounds of the plot [xmin, ymin, xmax, ymax]\n",
    "        indices: List[int] = None # Indices of the rows to plot\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the dataframe on a map with an OSM underlay.\n",
    "        \"\"\"\n",
    "\n",
    "        # Default to all indices if none are provided\n",
    "        if indices is None:\n",
    "            indices = self.gdf.index.values\n",
    "\n",
    "        if max_rows is not None and len(indices) > max_rows:\n",
    "            self.gdf = self.gdf.drop_duplicates(subset=[\"geometry\"])\n",
    "            rng = np.random.default_rng()\n",
    "            indices = rng.choice(indices, size=max_rows, replace=False)\n",
    "        ax = self.gdf.loc[indices].plot(\n",
    "                figsize=figsize, alpha=alpha, edgecolor='k', markersize=1\n",
    "            )\n",
    "\n",
    "        # If bounds are provided, set the bounds of the plot\n",
    "        if bounds is not None:\n",
    "            ax.set_xlim(bounds[0], bounds[2])\n",
    "            ax.set_ylim(bounds[1], bounds[3])\n",
    "\n",
    "        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "        ax.set_axis_off()\n",
    "        plt.show()\n",
    "\n",
    "    def fetch_and_plot_image(\n",
    "        self,\n",
    "        index: int,  # index of the row to plot\n",
    "        local_folder: Path,  # Local folder to save the image\n",
    "        force_fetch: bool,  # Whether to force fetching the image\n",
    "        bands: List[int] = [3, 4, 2],\n",
    "    ):  # Bands to read\n",
    "        \"\"\"\n",
    "        Fetches an image from a URL or local path, reads RGB bands, and plots it.\n",
    "        \"\"\"\n",
    "        row = self.gdf.loc[index]\n",
    "\n",
    "        if row[\"local_path\"] is None:\n",
    "            force_fetch = True\n",
    "\n",
    "        if force_fetch:\n",
    "            # print(f\"Fetching image for row {index}\")\n",
    "            url = row[\"source_url\"]\n",
    "            local_path = local_folder / Path(url).name\n",
    "            assert local_folder.exists(), f\"Local folder {local_path} does not exist\"\n",
    "            with rasterio.open(url) as src:\n",
    "                # print(f\"Reading {bands} bands from {url}\")\n",
    "                rgb = src.read(bands)\n",
    "            with rasterio.open(\n",
    "                local_path,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=rgb.shape[1],\n",
    "                width=rgb.shape[2],\n",
    "                count=len(bands),\n",
    "                dtype=rgb.dtype,\n",
    "                crs=src.crs,\n",
    "                transform=src.transform,\n",
    "            ) as dst:\n",
    "                # print(f\"Writing {bands} bands to {local_path}\")\n",
    "                dst.write(rgb)\n",
    "                self.gdf.loc[self.gdf[\"source_url\"] == url, \"local_path\"] = str(local_path)\n",
    "        else:\n",
    "            #print(f\"Reading local image for row {index}\")\n",
    "            local_path = row[\"local_path\"]\n",
    "            with rasterio.open(local_path) as src:\n",
    "                # print(f\"Reading {bands} bands from {local_path}\")\n",
    "                rgb = src.read()\n",
    "        rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "        rgb = np.transpose(rgb, [1, 2, 0])\n",
    "\n",
    "        #clip values on each band to 10-90 percentile\n",
    "        percentiles = np.percentile(rgb, [10, 90], axis=(0, 1))\n",
    "        rgb = np.clip(rgb, percentiles[0], percentiles[1])\n",
    "        return rgb\n",
    "\n",
    "    def rgb_imgs(\n",
    "        self,\n",
    "        row_indices: Union[int, List[int]],  # Indices of the rows to plot\n",
    "        local_folder: Path = None,  # Local folder to save the image\n",
    "        force_fetch: bool = False,\n",
    "        skip_plot: bool = False # skip plotting, but save them to local_folder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots RGB images for specified rows,\n",
    "        either from local storage or by fetching them.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(row_indices, int):\n",
    "            row_indices = [row_indices]\n",
    "\n",
    "        if \"local_path\" not in self.gdf.columns:\n",
    "            self.gdf[\"local_path\"] = None\n",
    "\n",
    "        if force_fetch and local_folder is None:\n",
    "            if self.gdf[\"local_path\"].notnull().any():\n",
    "                existing_files = self.gdf[self.gdf[\"local_path\"].notnull()].iloc[0]\n",
    "                local_folder = Path(existing_files[\"local_path\"]).parent\n",
    "            else:\n",
    "                raise ValueError(\"local_folder must be provided if force_fetch is True\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "            results = list(tqdm(executor.map(\n",
    "                lambda idx: self.fetch_and_plot_image(idx,\n",
    "                                                      local_folder,\n",
    "                                                      force_fetch),\n",
    "                                                      row_indices),\n",
    "                                                      total=len(row_indices)))\n",
    "\n",
    "        if not skip_plot:\n",
    "            self._plot_images(results, row_indices)\n",
    "\n",
    "    def _plot_images(self,\n",
    "                     images  , # list of images\n",
    "                     indices): # list of indices\n",
    "        \"\"\"\n",
    "        Plots the images from the results of the fetch_and_plot_image method.\n",
    "        \"\"\"\n",
    "        num_images = len(images)\n",
    "        num_cols = min(3, num_images)\n",
    "        num_rows = -(-num_images // num_cols)  # Ceiling division\n",
    "\n",
    "        # Create a figure and a set of subplots\n",
    "        fig, axes = plt.subplots(nrows=num_rows,\n",
    "                                 ncols=num_cols,\n",
    "                                 figsize=(5 * num_cols, 5 * num_rows))\n",
    "        axes = axes.flatten() if num_images > 1 else [axes]\n",
    "\n",
    "        for idx, image in enumerate(images):\n",
    "            if image is not None:\n",
    "                ax = axes[idx]\n",
    "                ax.imshow(image)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"Index: {indices[idx]}\")\n",
    "\n",
    "        # Turn off axes for any unused subplots\n",
    "        for ax in axes[num_images:]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def prep_posgres(self,\n",
    "                     db_url: str): # Postgres database URL\n",
    "        \"\"\"\n",
    "        Prepares a Postgres database for the embeddings.\n",
    "        \"\"\"\n",
    "        self.db_url = db_url\n",
    "        self.engine = create_engine(db_url) if db_url else None\n",
    "\n",
    "    def save_to_postgres(self):\n",
    "        \"\"\"\n",
    "        Saves the geodataframe to PostgreSQL.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.db_url is None or self.engine is None:\n",
    "            raise ValueError(\"Database URL not provided or engine not initialized.\"+\n",
    "                             \"call .prep_posgres(db_url) first.\")\n",
    "\n",
    "        # Convert 'embeddings' from list to numpy array\n",
    "        self.gdf['embeddings'] = self.gdf['embeddings'].apply(lambda x: np.array(x))\n",
    "\n",
    "        # Define column types for PostgreSQL\n",
    "        column_types = {\n",
    "            'geometry': Geometry('POLYGON', srid=3857),\n",
    "            'embeddings': ARRAY(FLOAT),\n",
    "            'source_url': VARCHAR,\n",
    "            'local_url': VARCHAR,\n",
    "            'date': DATE,\n",
    "            'x': FLOAT,\n",
    "            'y': FLOAT,\n",
    "            'location': VARCHAR,\n",
    "            'start_date': DATE,\n",
    "            'end_date': DATE,\n",
    "            'version': VARCHAR\n",
    "        }\n",
    "\n",
    "        #check that all keys in column_types are in gdf.columns and print missing keys\n",
    "        missing_keys = set(column_types.keys()) - set(self.gdf.columns)\n",
    "        if len(missing_keys) > 0:\n",
    "            print(f\"Missing keys: {missing_keys}\")\n",
    "            raise ValueError(\"Missing keys in gdf.columns\")\n",
    "\n",
    "        self.gdf.to_sql('embeddings', self.engine, if_exists='replace', index=False, dtype=column_types)\n",
    "\n",
    "        # Save to PostgreSQL\n",
    "        self.gdf.to_sql('embeddings', self.engine, if_exists='replace', index=False, dtype=column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EmbeddingsHandler` has several methods to help you work with embeddings.\n",
    "\n",
    "This is how you can load embeddings from a file or folder with files, including limiting the number of embeddings to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_doc(EmbeddingsHandler.read_geoparquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is how to read up to 10 random files from a folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5\n",
      " Merging dataframes...\n",
      "Done!\n",
      " Total rows:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = Path(\"../fixtures/sample_embedding/01WCP_20170701_20210603_v001.gpq\")\n",
    "embeddings = EmbeddingsHandler(embeddings_path, max_files=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can plot the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/showdoc.py:168\u001b[0m, in \u001b[0;36mBasicMarkdownRenderer._repr_markdown_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_markdown_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    167\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 168\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[43mNbdevLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m src: doc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _ext_link(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat:right; font-size:smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    170\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_level\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/doclinks.py:204\u001b[0m, in \u001b[0;36mNbdevLookup.__init__\u001b[0;34m(self, strip_libs, incl_libs, skip_mods)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: incl_libs \u001b[38;5;241m=\u001b[39m (L(incl_libs)\u001b[38;5;241m+\u001b[39mstrip_libs)\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Dict from lib name to _nbdev module for incl_libs (defaults to all)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_qual_syms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnbdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mincl_libs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mincl_libs\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    206\u001b[0m py_syms \u001b[38;5;241m=\u001b[39m merge(\u001b[38;5;241m*\u001b[39mL(o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mconcat())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m strip_libs:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/doclinks.py:204\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: incl_libs \u001b[38;5;241m=\u001b[39m (L(incl_libs)\u001b[38;5;241m+\u001b[39mstrip_libs)\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Dict from lib name to _nbdev module for incl_libs (defaults to all)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;241m=\u001b[39m {o\u001b[38;5;241m.\u001b[39mname: _qual_syms(\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(pkg_resources\u001b[38;5;241m.\u001b[39miter_entry_points(group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnbdev\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    205\u001b[0m                \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m o\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01min\u001b[39;00m incl_libs}\n\u001b[1;32m    206\u001b[0m py_syms \u001b[38;5;241m=\u001b[39m merge(\u001b[38;5;241m*\u001b[39mL(o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mconcat())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m strip_libs:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/pkg_resources/__init__.py:2522\u001b[0m, in \u001b[0;36mEntryPoint.resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;124;03m    Resolve the entry point from its module and attrs.\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2522\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__name__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2524\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mgetattr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs, module)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clay'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/showdoc.py:168\u001b[0m, in \u001b[0;36mBasicMarkdownRenderer._repr_markdown_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_markdown_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    167\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 168\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[43mNbdevLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m src: doc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _ext_link(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat:right; font-size:smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    170\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_level\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/doclinks.py:204\u001b[0m, in \u001b[0;36mNbdevLookup.__init__\u001b[0;34m(self, strip_libs, incl_libs, skip_mods)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: incl_libs \u001b[38;5;241m=\u001b[39m (L(incl_libs)\u001b[38;5;241m+\u001b[39mstrip_libs)\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Dict from lib name to _nbdev module for incl_libs (defaults to all)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_qual_syms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnbdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mincl_libs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mincl_libs\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    206\u001b[0m py_syms \u001b[38;5;241m=\u001b[39m merge(\u001b[38;5;241m*\u001b[39mL(o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mconcat())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m strip_libs:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/nbdev/doclinks.py:204\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: incl_libs \u001b[38;5;241m=\u001b[39m (L(incl_libs)\u001b[38;5;241m+\u001b[39mstrip_libs)\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Dict from lib name to _nbdev module for incl_libs (defaults to all)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;241m=\u001b[39m {o\u001b[38;5;241m.\u001b[39mname: _qual_syms(\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(pkg_resources\u001b[38;5;241m.\u001b[39miter_entry_points(group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnbdev\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    205\u001b[0m                \u001b[38;5;28;01mif\u001b[39;00m incl_libs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m o\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01min\u001b[39;00m incl_libs}\n\u001b[1;32m    206\u001b[0m py_syms \u001b[38;5;241m=\u001b[39m merge(\u001b[38;5;241m*\u001b[39mL(o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentries\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mconcat())\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m strip_libs:\n",
      "File \u001b[0;32m~/miniforge3/envs/claymodel/lib/python3.11/site-packages/pkg_resources/__init__.py:2522\u001b[0m, in \u001b[0;36mEntryPoint.resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;124;03m    Resolve the entry point from its module and attrs.\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2522\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_name, fromlist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m], level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2524\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mgetattr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs, module)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clay'"
     ]
    }
   ],
   "source": [
    "#show_doc(EmbeddingsHandler.plot_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.plot_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.plot_locations(indices=[0,1,2,3], max_rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total areas is too big, you can visualize the embeddings areas on detail zoomin in around one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of one geometry\n",
    "first_geometry = embeddings.gdf.loc[0].geometry\n",
    "# Create a 1km buffer around the first geometry\n",
    "buffer = first_geometry.buffer(100 * 1000)  # 100 x 1km\n",
    "\n",
    "bounds = buffer.bounds\n",
    "\n",
    "# Call the plot method with the bounds\n",
    "embeddings.plot_locations(bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using a transparency `alpha=0.2`. Different shades of darkness are locations where there are several embeddings stacked on top of each other, i.e. from different times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the RGB image for a given embedding, you can use the `rgb_imgs` method. the first time it will use the `S3` url location to pull only the RGB bands, then save it locally for faster later retrieval.\n",
    "\n",
    "You must specify the rows you want to retrieve, and if the first time, the output folder where to save the images, if it can't reuse an existing local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = Path(\"tmp/rgbs/\")\n",
    "local_path.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "embeddings.rgb_imgs(\n",
    "    [0,1,2], local_folder=Path(local_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the `local_folder` argument if you already have other local rgb saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.rgb_imgs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed you can `force_fetch` from the `S3` location again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.rgb_imgs(0, force_fetch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
